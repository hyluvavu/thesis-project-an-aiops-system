{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>carts_response_time</th>\n",
       "      <th>carts_cpu_usage</th>\n",
       "      <th>carts_memory_usage</th>\n",
       "      <th>carts_network_receive</th>\n",
       "      <th>carts_network_transmit</th>\n",
       "      <th>catalogue_response_time</th>\n",
       "      <th>catalogue_cpu_usage</th>\n",
       "      <th>catalogue_memory_usage</th>\n",
       "      <th>...</th>\n",
       "      <th>shipping_response_time</th>\n",
       "      <th>shipping_cpu_usage</th>\n",
       "      <th>shipping_memory_usage</th>\n",
       "      <th>shipping_network_receive</th>\n",
       "      <th>shipping_network_transmit</th>\n",
       "      <th>user_response_time</th>\n",
       "      <th>user_cpu_usage</th>\n",
       "      <th>user_memory_usage</th>\n",
       "      <th>user_network_receive</th>\n",
       "      <th>user_network_transmit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-12 07:10:00</td>\n",
       "      <td>0</td>\n",
       "      <td>12.476340940120815</td>\n",
       "      <td>0.18547890088321844</td>\n",
       "      <td>294682624</td>\n",
       "      <td>420056.65241080325</td>\n",
       "      <td>941.1799168082489</td>\n",
       "      <td>12.437493082457138</td>\n",
       "      <td>0.024590561765954505</td>\n",
       "      <td>6643712</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1944444444444446</td>\n",
       "      <td>0.0052111929537415535</td>\n",
       "      <td>270745600</td>\n",
       "      <td>7956.755630481314</td>\n",
       "      <td>12.539541227056619</td>\n",
       "      <td>8.95863983344905</td>\n",
       "      <td>0.04435522920582362</td>\n",
       "      <td>9068544</td>\n",
       "      <td>182688.9577416301</td>\n",
       "      <td>367.2433083742146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-12 07:10:05</td>\n",
       "      <td>0</td>\n",
       "      <td>12.041813036690137</td>\n",
       "      <td>0.18547890088321844</td>\n",
       "      <td>294682624</td>\n",
       "      <td>280896.1415444082</td>\n",
       "      <td>644.1618677138906</td>\n",
       "      <td>10.741431730237572</td>\n",
       "      <td>0.013942173067587412</td>\n",
       "      <td>6643712</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1944444444444446</td>\n",
       "      <td>0.0036999904237644504</td>\n",
       "      <td>270745600</td>\n",
       "      <td>5002.824262536873</td>\n",
       "      <td>7.921533923303835</td>\n",
       "      <td>8.739691614691575</td>\n",
       "      <td>0.04435522920582362</td>\n",
       "      <td>9072640</td>\n",
       "      <td>182688.9577416301</td>\n",
       "      <td>367.2433083742146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-12 07:10:10</td>\n",
       "      <td>0</td>\n",
       "      <td>12.041813036690137</td>\n",
       "      <td>0.18547890088321842</td>\n",
       "      <td>294682624</td>\n",
       "      <td>400070.4653230587</td>\n",
       "      <td>917.4570243034973</td>\n",
       "      <td>10.741431730237572</td>\n",
       "      <td>0.01595301507632577</td>\n",
       "      <td>6643712</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1944444444444446</td>\n",
       "      <td>0.005300124306459976</td>\n",
       "      <td>270741504</td>\n",
       "      <td>5665.722175923413</td>\n",
       "      <td>8.971174692799103</td>\n",
       "      <td>8.739691614691575</td>\n",
       "      <td>0.04435522920582362</td>\n",
       "      <td>9072640</td>\n",
       "      <td>182688.9577416301</td>\n",
       "      <td>367.2433083742146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-12 07:10:15</td>\n",
       "      <td>0</td>\n",
       "      <td>12.041813036690137</td>\n",
       "      <td>0.18547890088321844</td>\n",
       "      <td>294682624</td>\n",
       "      <td>400070.4653230587</td>\n",
       "      <td>917.4570243034973</td>\n",
       "      <td>10.10742904841405</td>\n",
       "      <td>0.017963857085064126</td>\n",
       "      <td>6643712</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1944444444444446</td>\n",
       "      <td>0.005300124306459976</td>\n",
       "      <td>270741504</td>\n",
       "      <td>6328.620089309952</td>\n",
       "      <td>10.02081546229437</td>\n",
       "      <td>8.61599513720038</td>\n",
       "      <td>0.04435522920582362</td>\n",
       "      <td>9072640</td>\n",
       "      <td>182688.9577416301</td>\n",
       "      <td>367.2433083742146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-09-12 07:10:20</td>\n",
       "      <td>0</td>\n",
       "      <td>12.041813036690137</td>\n",
       "      <td>0.18547890088321844</td>\n",
       "      <td>294682624</td>\n",
       "      <td>400070.4653230587</td>\n",
       "      <td>917.4570243034973</td>\n",
       "      <td>10.10742904841405</td>\n",
       "      <td>0.019067005011057993</td>\n",
       "      <td>6643712</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1944444444444446</td>\n",
       "      <td>0.005300124306459976</td>\n",
       "      <td>270741504</td>\n",
       "      <td>5847.223624608648</td>\n",
       "      <td>9.258566335486908</td>\n",
       "      <td>8.61599513720038</td>\n",
       "      <td>0.04435522920582362</td>\n",
       "      <td>9072640</td>\n",
       "      <td>182688.9577416301</td>\n",
       "      <td>367.2433083742146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7556</th>\n",
       "      <td>2023-09-12 17:39:40</td>\n",
       "      <td>0</td>\n",
       "      <td>20.82031641043803</td>\n",
       "      <td>0.19104964232241167</td>\n",
       "      <td>317607936</td>\n",
       "      <td>443524.9746633851</td>\n",
       "      <td>1012.0529897205735</td>\n",
       "      <td>21.12278270509978</td>\n",
       "      <td>0.02649505388578243</td>\n",
       "      <td>9170944</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0647058823515185</td>\n",
       "      <td>0.003925517166390261</td>\n",
       "      <td>302043136</td>\n",
       "      <td>4924.353845297719</td>\n",
       "      <td>8.001969949916528</td>\n",
       "      <td>13.088017955801233</td>\n",
       "      <td>0.04286331333640354</td>\n",
       "      <td>19046400</td>\n",
       "      <td>186938.31061952727</td>\n",
       "      <td>381.6906972150898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7557</th>\n",
       "      <td>2023-09-12 17:39:45</td>\n",
       "      <td>0</td>\n",
       "      <td>20.82031641043803</td>\n",
       "      <td>0.19104964232241167</td>\n",
       "      <td>317607936</td>\n",
       "      <td>443524.9746633851</td>\n",
       "      <td>1012.0529897205735</td>\n",
       "      <td>21.44362934363181</td>\n",
       "      <td>0.025807625430748833</td>\n",
       "      <td>9179136</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0647058823515185</td>\n",
       "      <td>0.003925517166390261</td>\n",
       "      <td>302043136</td>\n",
       "      <td>5549.335295863478</td>\n",
       "      <td>9.017551474680024</td>\n",
       "      <td>13.054920580108824</td>\n",
       "      <td>0.04286331333640354</td>\n",
       "      <td>19046400</td>\n",
       "      <td>186938.31061952727</td>\n",
       "      <td>381.6906972150898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7558</th>\n",
       "      <td>2023-09-12 17:39:50</td>\n",
       "      <td>0</td>\n",
       "      <td>20.82031641043803</td>\n",
       "      <td>0.19104964232241167</td>\n",
       "      <td>317607936</td>\n",
       "      <td>443524.97466338496</td>\n",
       "      <td>1012.0529897205734</td>\n",
       "      <td>21.44362934363181</td>\n",
       "      <td>0.025807625430748833</td>\n",
       "      <td>9179136</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0647058823515185</td>\n",
       "      <td>0.003925517166390261</td>\n",
       "      <td>302043136</td>\n",
       "      <td>5817.827327026526</td>\n",
       "      <td>9.45384529771842</td>\n",
       "      <td>13.054920580108824</td>\n",
       "      <td>0.04286331333640354</td>\n",
       "      <td>19046400</td>\n",
       "      <td>186938.3106195273</td>\n",
       "      <td>381.6906972150899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7559</th>\n",
       "      <td>2023-09-12 17:39:55</td>\n",
       "      <td>0</td>\n",
       "      <td>20.82031641043803</td>\n",
       "      <td>0.19104964232241167</td>\n",
       "      <td>317607936</td>\n",
       "      <td>443524.97466338496</td>\n",
       "      <td>1012.0529897205734</td>\n",
       "      <td>21.44362934363181</td>\n",
       "      <td>0.025807625430748833</td>\n",
       "      <td>9179136</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1396551724125383</td>\n",
       "      <td>0.003925517166390261</td>\n",
       "      <td>302043136</td>\n",
       "      <td>5192.8458764607685</td>\n",
       "      <td>8.438263772954926</td>\n",
       "      <td>13.054920580108824</td>\n",
       "      <td>0.04254098554611217</td>\n",
       "      <td>19054592</td>\n",
       "      <td>186938.3106195273</td>\n",
       "      <td>381.6906972150899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7560</th>\n",
       "      <td>2023-09-12 17:40:00</td>\n",
       "      <td>0</td>\n",
       "      <td>19.40022317443121</td>\n",
       "      <td>0.19104964232241167</td>\n",
       "      <td>317607936</td>\n",
       "      <td>429917.5043390222</td>\n",
       "      <td>977.7805901070291</td>\n",
       "      <td>21.44362934363181</td>\n",
       "      <td>0.025490265380698242</td>\n",
       "      <td>9166848</td>\n",
       "      <td>...</td>\n",
       "      <td>2.486792452831114</td>\n",
       "      <td>0.003925517166390261</td>\n",
       "      <td>302043136</td>\n",
       "      <td>4567.86442589501</td>\n",
       "      <td>7.422682248191429</td>\n",
       "      <td>13.317905288626177</td>\n",
       "      <td>0.04254098554611217</td>\n",
       "      <td>19054592</td>\n",
       "      <td>186938.3106195273</td>\n",
       "      <td>381.6906972150899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7561 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp  label carts_response_time      carts_cpu_usage  \\\n",
       "0    2023-09-12 07:10:00      0  12.476340940120815  0.18547890088321844   \n",
       "1    2023-09-12 07:10:05      0  12.041813036690137  0.18547890088321844   \n",
       "2    2023-09-12 07:10:10      0  12.041813036690137  0.18547890088321842   \n",
       "3    2023-09-12 07:10:15      0  12.041813036690137  0.18547890088321844   \n",
       "4    2023-09-12 07:10:20      0  12.041813036690137  0.18547890088321844   \n",
       "...                  ...    ...                 ...                  ...   \n",
       "7556 2023-09-12 17:39:40      0   20.82031641043803  0.19104964232241167   \n",
       "7557 2023-09-12 17:39:45      0   20.82031641043803  0.19104964232241167   \n",
       "7558 2023-09-12 17:39:50      0   20.82031641043803  0.19104964232241167   \n",
       "7559 2023-09-12 17:39:55      0   20.82031641043803  0.19104964232241167   \n",
       "7560 2023-09-12 17:40:00      0   19.40022317443121  0.19104964232241167   \n",
       "\n",
       "     carts_memory_usage carts_network_receive carts_network_transmit  \\\n",
       "0             294682624    420056.65241080325      941.1799168082489   \n",
       "1             294682624     280896.1415444082      644.1618677138906   \n",
       "2             294682624     400070.4653230587      917.4570243034973   \n",
       "3             294682624     400070.4653230587      917.4570243034973   \n",
       "4             294682624     400070.4653230587      917.4570243034973   \n",
       "...                 ...                   ...                    ...   \n",
       "7556          317607936     443524.9746633851     1012.0529897205735   \n",
       "7557          317607936     443524.9746633851     1012.0529897205735   \n",
       "7558          317607936    443524.97466338496     1012.0529897205734   \n",
       "7559          317607936    443524.97466338496     1012.0529897205734   \n",
       "7560          317607936     429917.5043390222      977.7805901070291   \n",
       "\n",
       "     catalogue_response_time   catalogue_cpu_usage catalogue_memory_usage  \\\n",
       "0         12.437493082457138  0.024590561765954505                6643712   \n",
       "1         10.741431730237572  0.013942173067587412                6643712   \n",
       "2         10.741431730237572   0.01595301507632577                6643712   \n",
       "3          10.10742904841405  0.017963857085064126                6643712   \n",
       "4          10.10742904841405  0.019067005011057993                6643712   \n",
       "...                      ...                   ...                    ...   \n",
       "7556       21.12278270509978   0.02649505388578243                9170944   \n",
       "7557       21.44362934363181  0.025807625430748833                9179136   \n",
       "7558       21.44362934363181  0.025807625430748833                9179136   \n",
       "7559       21.44362934363181  0.025807625430748833                9179136   \n",
       "7560       21.44362934363181  0.025490265380698242                9166848   \n",
       "\n",
       "      ... shipping_response_time     shipping_cpu_usage shipping_memory_usage  \\\n",
       "0     ...     2.1944444444444446  0.0052111929537415535             270745600   \n",
       "1     ...     2.1944444444444446  0.0036999904237644504             270745600   \n",
       "2     ...     2.1944444444444446   0.005300124306459976             270741504   \n",
       "3     ...     2.1944444444444446   0.005300124306459976             270741504   \n",
       "4     ...     2.1944444444444446   0.005300124306459976             270741504   \n",
       "...   ...                    ...                    ...                   ...   \n",
       "7556  ...     2.0647058823515185   0.003925517166390261             302043136   \n",
       "7557  ...     2.0647058823515185   0.003925517166390261             302043136   \n",
       "7558  ...     2.0647058823515185   0.003925517166390261             302043136   \n",
       "7559  ...     2.1396551724125383   0.003925517166390261             302043136   \n",
       "7560  ...      2.486792452831114   0.003925517166390261             302043136   \n",
       "\n",
       "     shipping_network_receive shipping_network_transmit  user_response_time  \\\n",
       "0           7956.755630481314        12.539541227056619    8.95863983344905   \n",
       "1           5002.824262536873         7.921533923303835   8.739691614691575   \n",
       "2           5665.722175923413         8.971174692799103   8.739691614691575   \n",
       "3           6328.620089309952         10.02081546229437    8.61599513720038   \n",
       "4           5847.223624608648         9.258566335486908    8.61599513720038   \n",
       "...                       ...                       ...                 ...   \n",
       "7556        4924.353845297719         8.001969949916528  13.088017955801233   \n",
       "7557        5549.335295863478         9.017551474680024  13.054920580108824   \n",
       "7558        5817.827327026526          9.45384529771842  13.054920580108824   \n",
       "7559       5192.8458764607685         8.438263772954926  13.054920580108824   \n",
       "7560         4567.86442589501         7.422682248191429  13.317905288626177   \n",
       "\n",
       "           user_cpu_usage user_memory_usage user_network_receive  \\\n",
       "0     0.04435522920582362           9068544    182688.9577416301   \n",
       "1     0.04435522920582362           9072640    182688.9577416301   \n",
       "2     0.04435522920582362           9072640    182688.9577416301   \n",
       "3     0.04435522920582362           9072640    182688.9577416301   \n",
       "4     0.04435522920582362           9072640    182688.9577416301   \n",
       "...                   ...               ...                  ...   \n",
       "7556  0.04286331333640354          19046400   186938.31061952727   \n",
       "7557  0.04286331333640354          19046400   186938.31061952727   \n",
       "7558  0.04286331333640354          19046400    186938.3106195273   \n",
       "7559  0.04254098554611217          19054592    186938.3106195273   \n",
       "7560  0.04254098554611217          19054592    186938.3106195273   \n",
       "\n",
       "     user_network_transmit  \n",
       "0        367.2433083742146  \n",
       "1        367.2433083742146  \n",
       "2        367.2433083742146  \n",
       "3        367.2433083742146  \n",
       "4        367.2433083742146  \n",
       "...                    ...  \n",
       "7556     381.6906972150898  \n",
       "7557     381.6906972150898  \n",
       "7558     381.6906972150899  \n",
       "7559     381.6906972150899  \n",
       "7560     381.6906972150899  \n",
       "\n",
       "[7561 rows x 37 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code is adapted from https://github.com/AXinx/ELBD\n",
    "\n",
    "# Load the data.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "labeled_data = pd.read_pickle('labeled_data.pkl')\n",
    "labeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>carts_response_time</th>\n",
       "      <th>carts_cpu_usage</th>\n",
       "      <th>carts_memory_usage</th>\n",
       "      <th>carts_network_receive</th>\n",
       "      <th>carts_network_transmit</th>\n",
       "      <th>catalogue_response_time</th>\n",
       "      <th>catalogue_cpu_usage</th>\n",
       "      <th>catalogue_memory_usage</th>\n",
       "      <th>...</th>\n",
       "      <th>shipping_response_time</th>\n",
       "      <th>shipping_cpu_usage</th>\n",
       "      <th>shipping_memory_usage</th>\n",
       "      <th>shipping_network_receive</th>\n",
       "      <th>shipping_network_transmit</th>\n",
       "      <th>user_response_time</th>\n",
       "      <th>user_cpu_usage</th>\n",
       "      <th>user_memory_usage</th>\n",
       "      <th>user_network_receive</th>\n",
       "      <th>user_network_transmit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-12 07:10:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.179846</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.406793</td>\n",
       "      <td>0.463301</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.032006</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.241949</td>\n",
       "      <td>0.332955</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.048603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886923</td>\n",
       "      <td>0.858229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-09-12 07:10:05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.179846</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.266304</td>\n",
       "      <td>0.310241</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.017841</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.149831</td>\n",
       "      <td>0.193231</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.048603</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.886923</td>\n",
       "      <td>0.858229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-09-12 07:10:10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.179846</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.386616</td>\n",
       "      <td>0.451076</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.020516</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.170504</td>\n",
       "      <td>0.224990</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.048603</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.886923</td>\n",
       "      <td>0.858229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-09-12 07:10:15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.179846</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.386616</td>\n",
       "      <td>0.451076</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.023191</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.256748</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.048603</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.886923</td>\n",
       "      <td>0.858229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-09-12 07:10:20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.179846</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.386616</td>\n",
       "      <td>0.451076</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.024658</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.176164</td>\n",
       "      <td>0.233685</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.048603</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.886923</td>\n",
       "      <td>0.858229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7556</th>\n",
       "      <td>2023-09-12 17:39:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.185650</td>\n",
       "      <td>0.067643</td>\n",
       "      <td>0.430486</td>\n",
       "      <td>0.499823</td>\n",
       "      <td>0.007171</td>\n",
       "      <td>0.034540</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.031105</td>\n",
       "      <td>0.147384</td>\n",
       "      <td>0.195665</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.046879</td>\n",
       "      <td>0.009616</td>\n",
       "      <td>0.908189</td>\n",
       "      <td>0.893271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7557</th>\n",
       "      <td>2023-09-12 17:39:45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.185650</td>\n",
       "      <td>0.067643</td>\n",
       "      <td>0.430486</td>\n",
       "      <td>0.499823</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.033625</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.031105</td>\n",
       "      <td>0.166874</td>\n",
       "      <td>0.226393</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.046879</td>\n",
       "      <td>0.009616</td>\n",
       "      <td>0.908189</td>\n",
       "      <td>0.893271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7558</th>\n",
       "      <td>2023-09-12 17:39:50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.185650</td>\n",
       "      <td>0.067643</td>\n",
       "      <td>0.430486</td>\n",
       "      <td>0.499823</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.033625</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.031105</td>\n",
       "      <td>0.175247</td>\n",
       "      <td>0.239594</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.046879</td>\n",
       "      <td>0.009616</td>\n",
       "      <td>0.908189</td>\n",
       "      <td>0.893271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7559</th>\n",
       "      <td>2023-09-12 17:39:55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.185650</td>\n",
       "      <td>0.067643</td>\n",
       "      <td>0.430486</td>\n",
       "      <td>0.499823</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.033625</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.031105</td>\n",
       "      <td>0.155757</td>\n",
       "      <td>0.208866</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.046506</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>0.908189</td>\n",
       "      <td>0.893271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7560</th>\n",
       "      <td>2023-09-12 17:40:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.185650</td>\n",
       "      <td>0.067643</td>\n",
       "      <td>0.416748</td>\n",
       "      <td>0.482162</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.031105</td>\n",
       "      <td>0.136267</td>\n",
       "      <td>0.178138</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.046506</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>0.908189</td>\n",
       "      <td>0.893271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7561 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp  label  carts_response_time  carts_cpu_usage  \\\n",
       "0    2023-09-12 07:10:00    0.0             0.000082         0.179846   \n",
       "1    2023-09-12 07:10:05    0.0             0.000049         0.179846   \n",
       "2    2023-09-12 07:10:10    0.0             0.000049         0.179846   \n",
       "3    2023-09-12 07:10:15    0.0             0.000049         0.179846   \n",
       "4    2023-09-12 07:10:20    0.0             0.000049         0.179846   \n",
       "...                  ...    ...                  ...              ...   \n",
       "7556 2023-09-12 17:39:40    0.0             0.000700         0.185650   \n",
       "7557 2023-09-12 17:39:45    0.0             0.000700         0.185650   \n",
       "7558 2023-09-12 17:39:50    0.0             0.000700         0.185650   \n",
       "7559 2023-09-12 17:39:55    0.0             0.000700         0.185650   \n",
       "7560 2023-09-12 17:40:00    0.0             0.000595         0.185650   \n",
       "\n",
       "      carts_memory_usage  carts_network_receive  carts_network_transmit  \\\n",
       "0               0.000871               0.406793                0.463301   \n",
       "1               0.000871               0.266304                0.310241   \n",
       "2               0.000871               0.386616                0.451076   \n",
       "3               0.000871               0.386616                0.451076   \n",
       "4               0.000871               0.386616                0.451076   \n",
       "...                  ...                    ...                     ...   \n",
       "7556            0.067643               0.430486                0.499823   \n",
       "7557            0.067643               0.430486                0.499823   \n",
       "7558            0.067643               0.430486                0.499823   \n",
       "7559            0.067643               0.430486                0.499823   \n",
       "7560            0.067643               0.416748                0.482162   \n",
       "\n",
       "      catalogue_response_time  catalogue_cpu_usage  catalogue_memory_usage  \\\n",
       "0                    0.003304             0.032006                0.000032   \n",
       "1                    0.002549             0.017841                0.000032   \n",
       "2                    0.002549             0.020516                0.000032   \n",
       "3                    0.002267             0.023191                0.000032   \n",
       "4                    0.002267             0.024658                0.000032   \n",
       "...                       ...                  ...                     ...   \n",
       "7556                 0.007171             0.034540                0.002483   \n",
       "7557                 0.007314             0.033625                0.002491   \n",
       "7558                 0.007314             0.033625                0.002491   \n",
       "7559                 0.007314             0.033625                0.002491   \n",
       "7560                 0.007314             0.033203                0.002479   \n",
       "\n",
       "      ...  shipping_response_time  shipping_cpu_usage  shipping_memory_usage  \\\n",
       "0     ...                0.004848            0.004752               0.002651   \n",
       "1     ...                0.004848            0.003207               0.002651   \n",
       "2     ...                0.004848            0.004843               0.002648   \n",
       "3     ...                0.004848            0.004843               0.002648   \n",
       "4     ...                0.004848            0.004843               0.002648   \n",
       "...   ...                     ...                 ...                    ...   \n",
       "7556  ...                0.004139            0.003437               0.031105   \n",
       "7557  ...                0.004139            0.003437               0.031105   \n",
       "7558  ...                0.004139            0.003437               0.031105   \n",
       "7559  ...                0.004549            0.003437               0.031105   \n",
       "7560  ...                0.006448            0.003437               0.031105   \n",
       "\n",
       "      shipping_network_receive  shipping_network_transmit  user_response_time  \\\n",
       "0                     0.241949                   0.332955            0.002224   \n",
       "1                     0.149831                   0.193231            0.002086   \n",
       "2                     0.170504                   0.224990            0.002086   \n",
       "3                     0.191176                   0.256748            0.002009   \n",
       "4                     0.176164                   0.233685            0.002009   \n",
       "...                        ...                        ...                 ...   \n",
       "7556                  0.147384                   0.195665            0.004810   \n",
       "7557                  0.166874                   0.226393            0.004790   \n",
       "7558                  0.175247                   0.239594            0.004790   \n",
       "7559                  0.155757                   0.208866            0.004790   \n",
       "7560                  0.136267                   0.178138            0.004954   \n",
       "\n",
       "      user_cpu_usage  user_memory_usage  user_network_receive  \\\n",
       "0           0.048603           0.000000              0.886923   \n",
       "1           0.048603           0.000004              0.886923   \n",
       "2           0.048603           0.000004              0.886923   \n",
       "3           0.048603           0.000004              0.886923   \n",
       "4           0.048603           0.000004              0.886923   \n",
       "...              ...                ...                   ...   \n",
       "7556        0.046879           0.009616              0.908189   \n",
       "7557        0.046879           0.009616              0.908189   \n",
       "7558        0.046879           0.009616              0.908189   \n",
       "7559        0.046506           0.009624              0.908189   \n",
       "7560        0.046506           0.009624              0.908189   \n",
       "\n",
       "      user_network_transmit  \n",
       "0                  0.858229  \n",
       "1                  0.858229  \n",
       "2                  0.858229  \n",
       "3                  0.858229  \n",
       "4                  0.858229  \n",
       "...                     ...  \n",
       "7556               0.893271  \n",
       "7557               0.893271  \n",
       "7558               0.893271  \n",
       "7559               0.893271  \n",
       "7560               0.893271  \n",
       "\n",
       "[7561 rows x 37 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the data.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = pd.DataFrame(scaler.fit_transform(labeled_data.drop(['timestamp'], axis=1)))\n",
    "normalized_data.columns = labeled_data.columns[1:]\n",
    "normalized_labeled_data = pd.merge(labeled_data.timestamp, normalized_data, left_index=True, right_index=True, how='left')\n",
    "normalized_labeled_data.fillna(method='ffill', inplace=True)  # Forward fill.\n",
    "normalized_labeled_data.fillna(method='bfill', inplace=True)  # Backward fill any remaining NaNs.\n",
    "normalized_labeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from time import time\n",
    "# Supress warnings for clean output.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.pca import PCA\n",
    "\n",
    "from pyod.utils.utility import standardizer\n",
    "from pyod.utils.utility import precision_n_scores\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from pyod.models.combination import aom, moa, average, maximization, majority_vote\n",
    "from pyod.utils.data import generate_data\n",
    "from pyod.utils.data import evaluate_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred_labels(y_test, test_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, test_scores)\n",
    "    cutoff = thresholds[np.argmax(tpr - fpr)]\n",
    "    pred_label = []\n",
    "    for each in test_scores:\n",
    "        if each > cutoff:\n",
    "            pred_label.append(1)\n",
    "        else:\n",
    "            pred_label.append(0)    \n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cal_eval(label, pred_label, scores):\n",
    "    pr = round(metrics.precision_score(label, pred_label, average='macro'), ndigits=4)\n",
    "    re = round(metrics.recall_score(label, pred_label, average='macro'), ndigits=4)\n",
    "    f1 = round(metrics.f1_score(label, pred_label, average='macro'), ndigits=4)\n",
    "    roc = round(roc_auc_score(label, scores), ndigits=4)\n",
    "    return pr, re, f1, roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = normalized_labeled_data.iloc[:, 1:]\n",
    "y_train = normalized_labeled_data['label']\n",
    "X_test = normalized_labeled_data.iloc[:, 1:]\n",
    "y_test = normalized_labeled_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base detector 1 is fitted for prediction\n",
      "Base detector 2 is fitted for prediction\n",
      "Base detector 3 is fitted for prediction\n",
      "Base detector 4 is fitted for prediction\n",
      "17.345399999999998\n"
     ]
    }
   ],
   "source": [
    "outliers_fraction = np.count_nonzero(y_train) / len(y_train)\n",
    "random_state = np.random.RandomState(42)\n",
    "classifiers = {\n",
    "        'Isolation Forest': IForest(contamination=outliers_fraction,\n",
    "                                    random_state=random_state),\n",
    "        'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n",
    "        'Local Outlier Factor (LOF)': LOF(\n",
    "            contamination=outliers_fraction),\n",
    "        'One-class SVM (OCSVM)': OCSVM(contamination=outliers_fraction)\n",
    "}\n",
    "\n",
    "n_clf = len(classifiers)\n",
    "train_scores = np.zeros([X_train.shape[0], n_clf])\n",
    "test_scores = np.zeros([X_test.shape[0], n_clf])\n",
    "p_labels = np.zeros([X_test.shape[0], n_clf])\n",
    "\n",
    "i = 0\n",
    "train_duration = 0\n",
    "test_duration = 0\n",
    "for clf_name, clf in classifiers.items():\n",
    "    t0 = time()\n",
    "    clf.fit(X_train)\n",
    "    t1 = time()\n",
    "    test_sccore = clf.decision_function(X_test)\n",
    "    t_t = time()\n",
    "    test_mean_score = np.nanmean(test_scores)\n",
    "    test_scores[np.isnan(test_scores)] = test_mean_score\n",
    "    test_scores[:, i] = test_sccore\n",
    "    p_labels[:, i] = pred_labels(y_test, test_sccore)\n",
    "    i += 1\n",
    "    print('Base detector %i is fitted for prediction' % i)\n",
    "    train_duration += round(t1 - t0, ndigits=4)\n",
    "    test_duration += round(t_t - t1, ndigits=4)\n",
    "    \n",
    "# standardize test score\n",
    "test_scores_norm = standardizer(test_scores)\n",
    "print(train_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(test_scores_norm, y_test, test_size=0.9, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensemble_mlp(df_columns):\n",
    "    enm_mlp_df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "    t2 = time()\n",
    "    model = models.Sequential()\n",
    "    # Input Layer\n",
    "    model.add(layers.Dense(4, activation = \"relu\", input_shape=(len(train_y), 4)))\n",
    "    model.add(layers.Dense(20, activation = \"relu\"))\n",
    "    model.add(layers.Dense(20, activation = \"relu\"))\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "    model.summary()\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer = \"adam\",\n",
    "        loss = \"binary_crossentropy\",\n",
    "        metrics = [\"accuracy\"]\n",
    "    )\n",
    "    model.fit(\n",
    "     train_x, train_y,\n",
    "     epochs = 100,\n",
    "     batch_size = 20,\n",
    "     validation_data = (train_x, train_y)\n",
    "    )\n",
    "    t3 = time()\n",
    "    pred_test = model.predict(test_scores_norm)\n",
    "    t4 = time()\n",
    "    train_time = t3 - t2\n",
    "    test_time = t4 - t3\n",
    "    train_time_mlp = round(train_duration + train_time, ndigits=4)\n",
    "    test_time_mlp = round(test_duration + test_time, ndigits=4)\n",
    "    \n",
    "    pred_nn = []\n",
    "    for each in pred_test:\n",
    "        if each[0] > 0.5:\n",
    "            pred_nn.append(1)\n",
    "        else:\n",
    "            pred_nn.append(0)\n",
    "    pr_mlp, re_mlp, f1_mlp, roc_mlp = cal_eval(y_test, pred_nn, pred_test)\n",
    "    enm_mlp = pd.DataFrame([pr_mlp, re_mlp, f1_mlp, roc_mlp, train_time_mlp, test_time_mlp]).transpose()\n",
    "    enm_mlp.columns = df_columns\n",
    "    enm_mlp_df = pd.concat([enm_mlp_df, enm_mlp], axis=0)\n",
    "    return pred_nn, enm_mlp_df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 756, 4)            20        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 756, 20)           100       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 756, 20)           420       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 756, 1)            21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 0.6360 - accuracy: 0.8485 WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "38/38 [==============================] - 41s 41ms/step - loss: 0.6339 - accuracy: 0.8458 - val_loss: 0.5445 - val_accuracy: 0.7857\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.5238 - accuracy: 0.7717 - val_loss: 0.4544 - val_accuracy: 0.7989\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.4365 - accuracy: 0.8274 - val_loss: 0.3976 - val_accuracy: 0.8730\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.4159 - accuracy: 0.8561 - val_loss: 0.3624 - val_accuracy: 0.8783\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3447 - accuracy: 0.8869 - val_loss: 0.3372 - val_accuracy: 0.8902\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3204 - accuracy: 0.8996 - val_loss: 0.3132 - val_accuracy: 0.8902\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2968 - accuracy: 0.8933 - val_loss: 0.2910 - val_accuracy: 0.8902\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3267 - accuracy: 0.8618 - val_loss: 0.2746 - val_accuracy: 0.8981\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2844 - accuracy: 0.9043 - val_loss: 0.2640 - val_accuracy: 0.9061\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2681 - accuracy: 0.9082 - val_loss: 0.2572 - val_accuracy: 0.9074\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2638 - accuracy: 0.9072 - val_loss: 0.2524 - val_accuracy: 0.9101\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2442 - accuracy: 0.9166 - val_loss: 0.2489 - val_accuracy: 0.9153\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2684 - accuracy: 0.9095 - val_loss: 0.2464 - val_accuracy: 0.9140\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2539 - accuracy: 0.9084 - val_loss: 0.2441 - val_accuracy: 0.9140\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2490 - accuracy: 0.9164 - val_loss: 0.2421 - val_accuracy: 0.9140\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2252 - accuracy: 0.9173 - val_loss: 0.2406 - val_accuracy: 0.9140\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2413 - accuracy: 0.9143 - val_loss: 0.2391 - val_accuracy: 0.9140\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2239 - accuracy: 0.9138 - val_loss: 0.2376 - val_accuracy: 0.9153\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2137 - accuracy: 0.9192 - val_loss: 0.2366 - val_accuracy: 0.9140\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2067 - accuracy: 0.9234 - val_loss: 0.2358 - val_accuracy: 0.9140\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2438 - accuracy: 0.9101 - val_loss: 0.2343 - val_accuracy: 0.9140\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2352 - accuracy: 0.9025 - val_loss: 0.2333 - val_accuracy: 0.9153\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2025 - accuracy: 0.9247 - val_loss: 0.2324 - val_accuracy: 0.9114\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2430 - accuracy: 0.9104 - val_loss: 0.2316 - val_accuracy: 0.9153\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2206 - accuracy: 0.9147 - val_loss: 0.2305 - val_accuracy: 0.9153\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1998 - accuracy: 0.9210 - val_loss: 0.2298 - val_accuracy: 0.9153\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2536 - accuracy: 0.9192 - val_loss: 0.2304 - val_accuracy: 0.9153\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2167 - accuracy: 0.9271 - val_loss: 0.2280 - val_accuracy: 0.9153\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2228 - accuracy: 0.9172 - val_loss: 0.2272 - val_accuracy: 0.9153\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2266 - accuracy: 0.9088 - val_loss: 0.2264 - val_accuracy: 0.9140\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2310 - accuracy: 0.9280 - val_loss: 0.2258 - val_accuracy: 0.9153\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2410 - accuracy: 0.9125 - val_loss: 0.2250 - val_accuracy: 0.9153\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1859 - accuracy: 0.9346 - val_loss: 0.2243 - val_accuracy: 0.9167\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1765 - accuracy: 0.9361 - val_loss: 0.2236 - val_accuracy: 0.9180\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.2078 - accuracy: 0.9257 - val_loss: 0.2230 - val_accuracy: 0.9167\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.2071 - accuracy: 0.9252 - val_loss: 0.2223 - val_accuracy: 0.9193\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2440 - accuracy: 0.9260 - val_loss: 0.2220 - val_accuracy: 0.9193\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2393 - accuracy: 0.9153 - val_loss: 0.2209 - val_accuracy: 0.9206\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2206 - accuracy: 0.9239 - val_loss: 0.2204 - val_accuracy: 0.9233\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2326 - accuracy: 0.9213 - val_loss: 0.2197 - val_accuracy: 0.9233\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2542 - accuracy: 0.9041 - val_loss: 0.2191 - val_accuracy: 0.9259\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2537 - accuracy: 0.9210 - val_loss: 0.2186 - val_accuracy: 0.9220\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1989 - accuracy: 0.9395 - val_loss: 0.2182 - val_accuracy: 0.9233\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1975 - accuracy: 0.9368 - val_loss: 0.2174 - val_accuracy: 0.9259\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2176 - accuracy: 0.9169 - val_loss: 0.2164 - val_accuracy: 0.9286\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2455 - accuracy: 0.9146 - val_loss: 0.2158 - val_accuracy: 0.9299\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2284 - accuracy: 0.9200 - val_loss: 0.2159 - val_accuracy: 0.9233\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1938 - accuracy: 0.9272 - val_loss: 0.2153 - val_accuracy: 0.9272\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2148 - accuracy: 0.9251 - val_loss: 0.2138 - val_accuracy: 0.9286\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2292 - accuracy: 0.9385 - val_loss: 0.2132 - val_accuracy: 0.9299\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2275 - accuracy: 0.9235 - val_loss: 0.2125 - val_accuracy: 0.9312\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2187 - accuracy: 0.9286 - val_loss: 0.2120 - val_accuracy: 0.9312\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2009 - accuracy: 0.9346 - val_loss: 0.2113 - val_accuracy: 0.9299\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2232 - accuracy: 0.9354 - val_loss: 0.2106 - val_accuracy: 0.9339\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.2335 - accuracy: 0.9225 - val_loss: 0.2097 - val_accuracy: 0.9339\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1856 - accuracy: 0.9384 - val_loss: 0.2092 - val_accuracy: 0.9339\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2228 - accuracy: 0.9340 - val_loss: 0.2079 - val_accuracy: 0.9312\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2064 - accuracy: 0.9292 - val_loss: 0.2070 - val_accuracy: 0.9312\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2162 - accuracy: 0.9346 - val_loss: 0.2077 - val_accuracy: 0.9286\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1801 - accuracy: 0.9360 - val_loss: 0.2052 - val_accuracy: 0.9339\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1803 - accuracy: 0.9419 - val_loss: 0.2041 - val_accuracy: 0.9339\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2081 - accuracy: 0.9289 - val_loss: 0.2038 - val_accuracy: 0.9325\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1793 - accuracy: 0.9449 - val_loss: 0.2031 - val_accuracy: 0.9312\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1972 - accuracy: 0.9301 - val_loss: 0.2005 - val_accuracy: 0.9365\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1632 - accuracy: 0.9500 - val_loss: 0.1993 - val_accuracy: 0.9365\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1835 - accuracy: 0.9306 - val_loss: 0.1981 - val_accuracy: 0.9365\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1938 - accuracy: 0.9393 - val_loss: 0.1969 - val_accuracy: 0.9378\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2361 - accuracy: 0.9279 - val_loss: 0.1955 - val_accuracy: 0.9378\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2070 - accuracy: 0.9265 - val_loss: 0.1950 - val_accuracy: 0.9378\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1859 - accuracy: 0.9369 - val_loss: 0.1930 - val_accuracy: 0.9392\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1964 - accuracy: 0.9395 - val_loss: 0.1919 - val_accuracy: 0.9378\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1791 - accuracy: 0.9408 - val_loss: 0.1910 - val_accuracy: 0.9405\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1543 - accuracy: 0.9494 - val_loss: 0.1907 - val_accuracy: 0.9392\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1738 - accuracy: 0.9433 - val_loss: 0.1892 - val_accuracy: 0.9405\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1981 - accuracy: 0.9323 - val_loss: 0.1877 - val_accuracy: 0.9418\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1802 - accuracy: 0.9461 - val_loss: 0.1866 - val_accuracy: 0.9418\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1943 - accuracy: 0.9418 - val_loss: 0.1859 - val_accuracy: 0.9444\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1676 - accuracy: 0.9484 - val_loss: 0.1844 - val_accuracy: 0.9418\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1955 - accuracy: 0.9383 - val_loss: 0.1834 - val_accuracy: 0.9444\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1783 - accuracy: 0.9432 - val_loss: 0.1820 - val_accuracy: 0.9444\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1793 - accuracy: 0.9463 - val_loss: 0.1815 - val_accuracy: 0.9431\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1677 - accuracy: 0.9499 - val_loss: 0.1800 - val_accuracy: 0.9444\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2077 - accuracy: 0.9360 - val_loss: 0.1789 - val_accuracy: 0.9444\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2003 - accuracy: 0.9420 - val_loss: 0.1776 - val_accuracy: 0.9458\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1813 - accuracy: 0.9382 - val_loss: 0.1769 - val_accuracy: 0.9471\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2017 - accuracy: 0.9435 - val_loss: 0.1753 - val_accuracy: 0.9458\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1817 - accuracy: 0.9448 - val_loss: 0.1743 - val_accuracy: 0.9471\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1727 - accuracy: 0.9515 - val_loss: 0.1731 - val_accuracy: 0.9484\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1652 - accuracy: 0.9487 - val_loss: 0.1722 - val_accuracy: 0.9484\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1986 - accuracy: 0.9395 - val_loss: 0.1718 - val_accuracy: 0.9471\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1800 - accuracy: 0.9403 - val_loss: 0.1697 - val_accuracy: 0.9497\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1557 - accuracy: 0.9532 - val_loss: 0.1687 - val_accuracy: 0.9497\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1841 - accuracy: 0.9462 - val_loss: 0.1674 - val_accuracy: 0.9497\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1527 - accuracy: 0.9563 - val_loss: 0.1662 - val_accuracy: 0.9511\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1837 - accuracy: 0.9380 - val_loss: 0.1655 - val_accuracy: 0.9497\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1523 - accuracy: 0.9519 - val_loss: 0.1638 - val_accuracy: 0.9524\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.9517 - val_loss: 0.1630 - val_accuracy: 0.9511\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1329 - accuracy: 0.9605 - val_loss: 0.1619 - val_accuracy: 0.9511\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1478 - accuracy: 0.9546 - val_loss: 0.1609 - val_accuracy: 0.9511\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1696 - accuracy: 0.9575 - val_loss: 0.1588 - val_accuracy: 0.9511\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "   Precision  Recall  F1 score     AUC  Train time  Test time\n",
      "0     0.9386  0.9148    0.9252  0.9705     81.3235    18.2572\n"
     ]
    }
   ],
   "source": [
    "df_columns = ['Precision', 'Recall', 'F1 score', 'AUC', 'Train time', 'Test time']\n",
    "pred_nn, enm_mlp_df, model = ensemble_mlp(df_columns)\n",
    "# Display the evaluation results.\n",
    "print(enm_mlp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 756, 4)            20        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 756, 20)           100       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 756, 20)           420       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 756, 1)            21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      " 1/38 [..............................] - ETA: 8:02 - loss: 0.6927 - accuracy: 0.4000WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "38/38 [==============================] - 14s 13ms/step - loss: 0.6655 - accuracy: 0.6124 - val_loss: 0.6188 - val_accuracy: 0.7923\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6151 - accuracy: 0.7754 - val_loss: 0.5526 - val_accuracy: 0.8016\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5363 - accuracy: 0.7835 - val_loss: 0.4754 - val_accuracy: 0.8175\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4501 - accuracy: 0.8199 - val_loss: 0.4191 - val_accuracy: 0.8214\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.8242 - val_loss: 0.3866 - val_accuracy: 0.8333\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3805 - accuracy: 0.8293 - val_loss: 0.3583 - val_accuracy: 0.8492\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3848 - accuracy: 0.8319 - val_loss: 0.3342 - val_accuracy: 0.8690\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2850 - accuracy: 0.9007 - val_loss: 0.3156 - val_accuracy: 0.8810\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3378 - accuracy: 0.8716 - val_loss: 0.3031 - val_accuracy: 0.8849\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2792 - accuracy: 0.9024 - val_loss: 0.2951 - val_accuracy: 0.8889\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3317 - accuracy: 0.8672 - val_loss: 0.2900 - val_accuracy: 0.8942\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2934 - accuracy: 0.8888 - val_loss: 0.2839 - val_accuracy: 0.8968\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8957 - val_loss: 0.2801 - val_accuracy: 0.9021\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2738 - accuracy: 0.8942 - val_loss: 0.2768 - val_accuracy: 0.9061\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2787 - accuracy: 0.9063 - val_loss: 0.2742 - val_accuracy: 0.9034\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.9015 - val_loss: 0.2708 - val_accuracy: 0.9074\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2486 - accuracy: 0.9172 - val_loss: 0.2683 - val_accuracy: 0.9101\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2593 - accuracy: 0.9221 - val_loss: 0.2662 - val_accuracy: 0.9087\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2434 - accuracy: 0.9204 - val_loss: 0.2638 - val_accuracy: 0.9087\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.9143 - val_loss: 0.2622 - val_accuracy: 0.9074\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2502 - accuracy: 0.9209 - val_loss: 0.2604 - val_accuracy: 0.9074\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2553 - accuracy: 0.9135 - val_loss: 0.2588 - val_accuracy: 0.9087\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2639 - accuracy: 0.9083 - val_loss: 0.2573 - val_accuracy: 0.9087\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2285 - accuracy: 0.9169 - val_loss: 0.2559 - val_accuracy: 0.9074\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8805 - val_loss: 0.2546 - val_accuracy: 0.9061\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2510 - accuracy: 0.9129 - val_loss: 0.2536 - val_accuracy: 0.9087\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2729 - accuracy: 0.8972 - val_loss: 0.2521 - val_accuracy: 0.9087\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2566 - accuracy: 0.8986 - val_loss: 0.2509 - val_accuracy: 0.9061\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2465 - accuracy: 0.8971 - val_loss: 0.2496 - val_accuracy: 0.9074\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2068 - accuracy: 0.9196 - val_loss: 0.2482 - val_accuracy: 0.9087\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2647 - accuracy: 0.8978 - val_loss: 0.2469 - val_accuracy: 0.9074\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2367 - accuracy: 0.9130 - val_loss: 0.2458 - val_accuracy: 0.9087\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.9219 - val_loss: 0.2447 - val_accuracy: 0.9074\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2334 - accuracy: 0.9132 - val_loss: 0.2434 - val_accuracy: 0.9087\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9126 - val_loss: 0.2421 - val_accuracy: 0.9087\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2462 - accuracy: 0.9128 - val_loss: 0.2407 - val_accuracy: 0.9087\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2334 - accuracy: 0.9077 - val_loss: 0.2396 - val_accuracy: 0.9101\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2455 - accuracy: 0.9042 - val_loss: 0.2386 - val_accuracy: 0.9087\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2250 - accuracy: 0.9039 - val_loss: 0.2373 - val_accuracy: 0.9074\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2249 - accuracy: 0.9080 - val_loss: 0.2361 - val_accuracy: 0.9061\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9086 - val_loss: 0.2350 - val_accuracy: 0.9074\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2270 - accuracy: 0.9107 - val_loss: 0.2339 - val_accuracy: 0.9074\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2073 - accuracy: 0.9253 - val_loss: 0.2325 - val_accuracy: 0.9061\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2242 - accuracy: 0.9094 - val_loss: 0.2305 - val_accuracy: 0.9074\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2358 - accuracy: 0.9182 - val_loss: 0.2289 - val_accuracy: 0.9087\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.9121 - val_loss: 0.2281 - val_accuracy: 0.9101\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2366 - accuracy: 0.9112 - val_loss: 0.2265 - val_accuracy: 0.9101\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2285 - accuracy: 0.9039 - val_loss: 0.2254 - val_accuracy: 0.9101\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2400 - accuracy: 0.9088 - val_loss: 0.2244 - val_accuracy: 0.9087\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2076 - accuracy: 0.9138 - val_loss: 0.2234 - val_accuracy: 0.9074\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2097 - accuracy: 0.9117 - val_loss: 0.2219 - val_accuracy: 0.9074\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2077 - accuracy: 0.9274 - val_loss: 0.2204 - val_accuracy: 0.9087\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.9041 - val_loss: 0.2191 - val_accuracy: 0.9087\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2317 - accuracy: 0.9021 - val_loss: 0.2184 - val_accuracy: 0.9101\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2238 - accuracy: 0.9056 - val_loss: 0.2171 - val_accuracy: 0.9114\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.9013 - val_loss: 0.2159 - val_accuracy: 0.9114\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1997 - accuracy: 0.9172 - val_loss: 0.2146 - val_accuracy: 0.9140\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2200 - accuracy: 0.9123 - val_loss: 0.2133 - val_accuracy: 0.9153\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1912 - accuracy: 0.9207 - val_loss: 0.2120 - val_accuracy: 0.9127\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2271 - accuracy: 0.9068 - val_loss: 0.2111 - val_accuracy: 0.9167\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2421 - accuracy: 0.9049 - val_loss: 0.2094 - val_accuracy: 0.9153\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2052 - accuracy: 0.9156 - val_loss: 0.2084 - val_accuracy: 0.9180\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.9312 - val_loss: 0.2073 - val_accuracy: 0.9167\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9246 - val_loss: 0.2057 - val_accuracy: 0.9180\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1937 - accuracy: 0.9301 - val_loss: 0.2047 - val_accuracy: 0.9180\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2287 - accuracy: 0.9101 - val_loss: 0.2030 - val_accuracy: 0.9206\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1885 - accuracy: 0.9266 - val_loss: 0.2017 - val_accuracy: 0.9167\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2254 - accuracy: 0.9129 - val_loss: 0.2002 - val_accuracy: 0.9180\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.9032 - val_loss: 0.1991 - val_accuracy: 0.9193\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1971 - accuracy: 0.9305 - val_loss: 0.1983 - val_accuracy: 0.9206\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1922 - accuracy: 0.9241 - val_loss: 0.1973 - val_accuracy: 0.9259\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1668 - accuracy: 0.9363 - val_loss: 0.1964 - val_accuracy: 0.9272\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2009 - accuracy: 0.9290 - val_loss: 0.1950 - val_accuracy: 0.9272\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.9192 - val_loss: 0.1945 - val_accuracy: 0.9286\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1786 - accuracy: 0.9311 - val_loss: 0.1935 - val_accuracy: 0.9286\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1710 - accuracy: 0.9392 - val_loss: 0.1931 - val_accuracy: 0.9259\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2024 - accuracy: 0.9350 - val_loss: 0.1922 - val_accuracy: 0.9286\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2118 - accuracy: 0.9110 - val_loss: 0.1909 - val_accuracy: 0.9286\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1685 - accuracy: 0.9384 - val_loss: 0.1902 - val_accuracy: 0.9299\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9126 - val_loss: 0.1900 - val_accuracy: 0.9299\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1671 - accuracy: 0.9381 - val_loss: 0.1888 - val_accuracy: 0.9286\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1965 - accuracy: 0.9183 - val_loss: 0.1885 - val_accuracy: 0.9299\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.9201 - val_loss: 0.1877 - val_accuracy: 0.9325\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1794 - accuracy: 0.9377 - val_loss: 0.1874 - val_accuracy: 0.9299\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.9277 - val_loss: 0.1874 - val_accuracy: 0.9299\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1820 - accuracy: 0.9241 - val_loss: 0.1857 - val_accuracy: 0.9286\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1401 - accuracy: 0.9428 - val_loss: 0.1850 - val_accuracy: 0.9312\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1653 - accuracy: 0.9420 - val_loss: 0.1844 - val_accuracy: 0.9325\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1827 - accuracy: 0.9343 - val_loss: 0.1839 - val_accuracy: 0.9325\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1831 - accuracy: 0.9356 - val_loss: 0.1830 - val_accuracy: 0.9312\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1882 - accuracy: 0.9314 - val_loss: 0.1826 - val_accuracy: 0.9339\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1902 - accuracy: 0.9237 - val_loss: 0.1821 - val_accuracy: 0.9339\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1789 - accuracy: 0.9357 - val_loss: 0.1814 - val_accuracy: 0.9352\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1635 - accuracy: 0.9491 - val_loss: 0.1809 - val_accuracy: 0.9365\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.9344 - val_loss: 0.1802 - val_accuracy: 0.9365\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1612 - accuracy: 0.9437 - val_loss: 0.1801 - val_accuracy: 0.9378\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.9293 - val_loss: 0.1794 - val_accuracy: 0.9339\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2134 - accuracy: 0.9262 - val_loss: 0.1788 - val_accuracy: 0.9392\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1766 - accuracy: 0.9408 - val_loss: 0.1781 - val_accuracy: 0.9378\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1774 - accuracy: 0.9434 - val_loss: 0.1774 - val_accuracy: 0.9365\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 756, 4)            20        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 756, 20)           100       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 756, 20)           420       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 756, 1)            21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_4_input'), name='dense_4_input', description=\"created by layer 'dense_4_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_4_input'), name='dense_4_input', description=\"created by layer 'dense_4_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      " 1/38 [..............................] - ETA: 26s - loss: 0.6673 - accuracy: 0.7000WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_4_input'), name='dense_4_input', description=\"created by layer 'dense_4_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "38/38 [==============================] - 1s 8ms/step - loss: 0.6494 - accuracy: 0.7128 - val_loss: 0.5600 - val_accuracy: 0.7791\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.8043 - val_loss: 0.4915 - val_accuracy: 0.8122\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.8094 - val_loss: 0.4489 - val_accuracy: 0.8360\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4513 - accuracy: 0.8347 - val_loss: 0.4147 - val_accuracy: 0.8492\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.8582 - val_loss: 0.3841 - val_accuracy: 0.8624\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.8889 - val_loss: 0.3564 - val_accuracy: 0.8743\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8786 - val_loss: 0.3326 - val_accuracy: 0.8810\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3297 - accuracy: 0.8840 - val_loss: 0.3121 - val_accuracy: 0.8849\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2819 - accuracy: 0.8989 - val_loss: 0.2915 - val_accuracy: 0.8955\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8960 - val_loss: 0.2783 - val_accuracy: 0.9021\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2991 - accuracy: 0.8836 - val_loss: 0.2665 - val_accuracy: 0.9034\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.9063 - val_loss: 0.2589 - val_accuracy: 0.9008\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2412 - accuracy: 0.9108 - val_loss: 0.2529 - val_accuracy: 0.9074\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2337 - accuracy: 0.9209 - val_loss: 0.2481 - val_accuracy: 0.9061\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9194 - val_loss: 0.2444 - val_accuracy: 0.9087\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9036 - val_loss: 0.2410 - val_accuracy: 0.9074\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2138 - accuracy: 0.9124 - val_loss: 0.2381 - val_accuracy: 0.9101\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.9085 - val_loss: 0.2357 - val_accuracy: 0.9127\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9092 - val_loss: 0.2336 - val_accuracy: 0.9127\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2321 - accuracy: 0.9183 - val_loss: 0.2312 - val_accuracy: 0.9193\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2172 - accuracy: 0.9211 - val_loss: 0.2292 - val_accuracy: 0.9233\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2381 - accuracy: 0.9124 - val_loss: 0.2274 - val_accuracy: 0.9220\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2188 - accuracy: 0.9149 - val_loss: 0.2258 - val_accuracy: 0.9220\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2526 - accuracy: 0.9170 - val_loss: 0.2242 - val_accuracy: 0.9206\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9299 - val_loss: 0.2229 - val_accuracy: 0.9167\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.9153 - val_loss: 0.2215 - val_accuracy: 0.9220\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2497 - accuracy: 0.9187 - val_loss: 0.2202 - val_accuracy: 0.9233\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2193 - accuracy: 0.9206 - val_loss: 0.2185 - val_accuracy: 0.9259\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2001 - accuracy: 0.9299 - val_loss: 0.2169 - val_accuracy: 0.9272\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9242 - val_loss: 0.2156 - val_accuracy: 0.9286\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2127 - accuracy: 0.9284 - val_loss: 0.2140 - val_accuracy: 0.9272\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9358 - val_loss: 0.2125 - val_accuracy: 0.9286\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.9177 - val_loss: 0.2111 - val_accuracy: 0.9259\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2400 - accuracy: 0.9187 - val_loss: 0.2099 - val_accuracy: 0.9233\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1889 - accuracy: 0.9379 - val_loss: 0.2083 - val_accuracy: 0.9259\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1938 - accuracy: 0.9375 - val_loss: 0.2071 - val_accuracy: 0.9259\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2039 - accuracy: 0.9254 - val_loss: 0.2056 - val_accuracy: 0.9259\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1778 - accuracy: 0.9398 - val_loss: 0.2044 - val_accuracy: 0.9272\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2020 - accuracy: 0.9319 - val_loss: 0.2030 - val_accuracy: 0.9259\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1853 - accuracy: 0.9314 - val_loss: 0.2016 - val_accuracy: 0.9259\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2097 - accuracy: 0.9181 - val_loss: 0.2002 - val_accuracy: 0.9286\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2273 - accuracy: 0.9203 - val_loss: 0.1990 - val_accuracy: 0.9272\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1943 - accuracy: 0.9254 - val_loss: 0.1978 - val_accuracy: 0.9299\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1908 - accuracy: 0.9270 - val_loss: 0.1966 - val_accuracy: 0.9312\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.9381 - val_loss: 0.1956 - val_accuracy: 0.9312\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2171 - accuracy: 0.9281 - val_loss: 0.1942 - val_accuracy: 0.9312\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1648 - accuracy: 0.9426 - val_loss: 0.1933 - val_accuracy: 0.9339\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1889 - accuracy: 0.9313 - val_loss: 0.1922 - val_accuracy: 0.9325\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1806 - accuracy: 0.9368 - val_loss: 0.1908 - val_accuracy: 0.9339\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1840 - accuracy: 0.9309 - val_loss: 0.1901 - val_accuracy: 0.9339\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1832 - accuracy: 0.9379 - val_loss: 0.1889 - val_accuracy: 0.9312\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1912 - accuracy: 0.9390 - val_loss: 0.1878 - val_accuracy: 0.9352\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1968 - accuracy: 0.9326 - val_loss: 0.1868 - val_accuracy: 0.9365\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1853 - accuracy: 0.9334 - val_loss: 0.1859 - val_accuracy: 0.9365\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1741 - accuracy: 0.9386 - val_loss: 0.1851 - val_accuracy: 0.9392\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2123 - accuracy: 0.9319 - val_loss: 0.1841 - val_accuracy: 0.9378\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9394 - val_loss: 0.1832 - val_accuracy: 0.9365\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2006 - accuracy: 0.9380 - val_loss: 0.1826 - val_accuracy: 0.9352\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1694 - accuracy: 0.9326 - val_loss: 0.1811 - val_accuracy: 0.9378\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1902 - accuracy: 0.9350 - val_loss: 0.1808 - val_accuracy: 0.9378\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1781 - accuracy: 0.9336 - val_loss: 0.1793 - val_accuracy: 0.9365\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1984 - accuracy: 0.9292 - val_loss: 0.1786 - val_accuracy: 0.9365\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.9303 - val_loss: 0.1774 - val_accuracy: 0.9378\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1634 - accuracy: 0.9380 - val_loss: 0.1763 - val_accuracy: 0.9392\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1726 - accuracy: 0.9382 - val_loss: 0.1752 - val_accuracy: 0.9405\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1331 - accuracy: 0.9578 - val_loss: 0.1743 - val_accuracy: 0.9392\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1610 - accuracy: 0.9397 - val_loss: 0.1733 - val_accuracy: 0.9418\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2023 - accuracy: 0.9328 - val_loss: 0.1727 - val_accuracy: 0.9405\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1813 - accuracy: 0.9355 - val_loss: 0.1714 - val_accuracy: 0.9405\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1942 - accuracy: 0.9328 - val_loss: 0.1705 - val_accuracy: 0.9418\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1900 - accuracy: 0.9349 - val_loss: 0.1694 - val_accuracy: 0.9392\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1959 - accuracy: 0.9399 - val_loss: 0.1678 - val_accuracy: 0.9418\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1567 - accuracy: 0.9462 - val_loss: 0.1676 - val_accuracy: 0.9444\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1912 - accuracy: 0.9333 - val_loss: 0.1657 - val_accuracy: 0.9431\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1473 - accuracy: 0.9554 - val_loss: 0.1652 - val_accuracy: 0.9444\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1688 - accuracy: 0.9469 - val_loss: 0.1636 - val_accuracy: 0.9458\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.9443 - val_loss: 0.1629 - val_accuracy: 0.9431\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1573 - accuracy: 0.9366 - val_loss: 0.1616 - val_accuracy: 0.9458\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1526 - accuracy: 0.9491 - val_loss: 0.1609 - val_accuracy: 0.9458\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1442 - accuracy: 0.9492 - val_loss: 0.1601 - val_accuracy: 0.9444\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1398 - accuracy: 0.9539 - val_loss: 0.1589 - val_accuracy: 0.9458\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1751 - accuracy: 0.9381 - val_loss: 0.1577 - val_accuracy: 0.9471\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1680 - accuracy: 0.9454 - val_loss: 0.1566 - val_accuracy: 0.9497\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1709 - accuracy: 0.9383 - val_loss: 0.1555 - val_accuracy: 0.9497\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1608 - accuracy: 0.9473 - val_loss: 0.1545 - val_accuracy: 0.9497\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1448 - accuracy: 0.9530 - val_loss: 0.1535 - val_accuracy: 0.9511\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1239 - accuracy: 0.9569 - val_loss: 0.1517 - val_accuracy: 0.9511\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1467 - accuracy: 0.9561 - val_loss: 0.1507 - val_accuracy: 0.9511\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1582 - accuracy: 0.9435 - val_loss: 0.1490 - val_accuracy: 0.9511\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1476 - accuracy: 0.9535 - val_loss: 0.1476 - val_accuracy: 0.9524\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1612 - accuracy: 0.9442 - val_loss: 0.1463 - val_accuracy: 0.9524\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1123 - accuracy: 0.9702 - val_loss: 0.1454 - val_accuracy: 0.9511\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1438 - accuracy: 0.9468 - val_loss: 0.1427 - val_accuracy: 0.9524\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9524 - val_loss: 0.1410 - val_accuracy: 0.9524\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1445 - accuracy: 0.9513 - val_loss: 0.1392 - val_accuracy: 0.9524\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9466 - val_loss: 0.1375 - val_accuracy: 0.9524\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1399 - accuracy: 0.9508 - val_loss: 0.1356 - val_accuracy: 0.9524\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1620 - accuracy: 0.9442 - val_loss: 0.1356 - val_accuracy: 0.9511\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1457 - accuracy: 0.9536 - val_loss: 0.1313 - val_accuracy: 0.9537\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1183 - accuracy: 0.9647 - val_loss: 0.1296 - val_accuracy: 0.9550\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_4_input'), name='dense_4_input', description=\"created by layer 'dense_4_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "2\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 756, 4)            20        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 756, 20)           100       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 756, 20)           420       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 756, 1)            21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_8_input'), name='dense_8_input', description=\"created by layer 'dense_8_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_8_input'), name='dense_8_input', description=\"created by layer 'dense_8_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      " 1/38 [..............................] - ETA: 25s - loss: 0.6863 - accuracy: 0.5500WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_8_input'), name='dense_8_input', description=\"created by layer 'dense_8_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 0.7023 - accuracy: 0.4966 - val_loss: 0.6071 - val_accuracy: 0.8770\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5675 - accuracy: 0.8861 - val_loss: 0.4840 - val_accuracy: 0.8823\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4541 - accuracy: 0.8966 - val_loss: 0.3882 - val_accuracy: 0.8810\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3503 - accuracy: 0.9010 - val_loss: 0.3332 - val_accuracy: 0.8876\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3688 - accuracy: 0.8698 - val_loss: 0.3073 - val_accuracy: 0.8902\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2936 - accuracy: 0.8972 - val_loss: 0.2909 - val_accuracy: 0.8955\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2529 - accuracy: 0.9124 - val_loss: 0.2804 - val_accuracy: 0.9008\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.8989 - val_loss: 0.2731 - val_accuracy: 0.9021\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.9069 - val_loss: 0.2678 - val_accuracy: 0.9021\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.9112 - val_loss: 0.2641 - val_accuracy: 0.9061\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.9051 - val_loss: 0.2596 - val_accuracy: 0.9061\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2524 - accuracy: 0.9174 - val_loss: 0.2562 - val_accuracy: 0.9074\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2428 - accuracy: 0.9180 - val_loss: 0.2536 - val_accuracy: 0.9087\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2920 - accuracy: 0.9035 - val_loss: 0.2516 - val_accuracy: 0.9087\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.9138 - val_loss: 0.2496 - val_accuracy: 0.9101\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.9080 - val_loss: 0.2478 - val_accuracy: 0.9087\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2251 - accuracy: 0.9123 - val_loss: 0.2463 - val_accuracy: 0.9101\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2206 - accuracy: 0.9170 - val_loss: 0.2450 - val_accuracy: 0.9127\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2089 - accuracy: 0.9188 - val_loss: 0.2437 - val_accuracy: 0.9127\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2154 - accuracy: 0.9215 - val_loss: 0.2426 - val_accuracy: 0.9127\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2495 - accuracy: 0.9081 - val_loss: 0.2415 - val_accuracy: 0.9087\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2384 - accuracy: 0.9205 - val_loss: 0.2405 - val_accuracy: 0.9101\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2449 - accuracy: 0.9079 - val_loss: 0.2398 - val_accuracy: 0.9101\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.9062 - val_loss: 0.2388 - val_accuracy: 0.9114\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.9054 - val_loss: 0.2377 - val_accuracy: 0.9101\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.9188 - val_loss: 0.2371 - val_accuracy: 0.9114\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9143 - val_loss: 0.2361 - val_accuracy: 0.9153\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2784 - accuracy: 0.9039 - val_loss: 0.2351 - val_accuracy: 0.9127\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2373 - accuracy: 0.9240 - val_loss: 0.2342 - val_accuracy: 0.9114\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2373 - accuracy: 0.9128 - val_loss: 0.2332 - val_accuracy: 0.9167\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2157 - accuracy: 0.9241 - val_loss: 0.2322 - val_accuracy: 0.9140\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2699 - accuracy: 0.8947 - val_loss: 0.2310 - val_accuracy: 0.9153\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.9255 - val_loss: 0.2301 - val_accuracy: 0.9180\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2417 - accuracy: 0.9257 - val_loss: 0.2296 - val_accuracy: 0.9206\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9279 - val_loss: 0.2285 - val_accuracy: 0.9206\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2264 - accuracy: 0.9131 - val_loss: 0.2277 - val_accuracy: 0.9193\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9175 - val_loss: 0.2268 - val_accuracy: 0.9206\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2174 - accuracy: 0.9199 - val_loss: 0.2260 - val_accuracy: 0.9233\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2036 - accuracy: 0.9291 - val_loss: 0.2249 - val_accuracy: 0.9206\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2467 - accuracy: 0.9044 - val_loss: 0.2238 - val_accuracy: 0.9206\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2170 - accuracy: 0.9228 - val_loss: 0.2230 - val_accuracy: 0.9220\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2200 - accuracy: 0.9271 - val_loss: 0.2220 - val_accuracy: 0.9220\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2108 - accuracy: 0.9123 - val_loss: 0.2213 - val_accuracy: 0.9220\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2031 - accuracy: 0.9262 - val_loss: 0.2202 - val_accuracy: 0.9206\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2183 - accuracy: 0.9199 - val_loss: 0.2192 - val_accuracy: 0.9206\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2021 - accuracy: 0.9294 - val_loss: 0.2183 - val_accuracy: 0.9206\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2021 - accuracy: 0.9259 - val_loss: 0.2173 - val_accuracy: 0.9193\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2542 - accuracy: 0.9049 - val_loss: 0.2163 - val_accuracy: 0.9206\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2140 - accuracy: 0.9271 - val_loss: 0.2154 - val_accuracy: 0.9193\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9179 - val_loss: 0.2150 - val_accuracy: 0.9193\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2324 - accuracy: 0.9226 - val_loss: 0.2137 - val_accuracy: 0.9206\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.9115 - val_loss: 0.2125 - val_accuracy: 0.9206\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2188 - accuracy: 0.9079 - val_loss: 0.2112 - val_accuracy: 0.9206\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2398 - accuracy: 0.9217 - val_loss: 0.2102 - val_accuracy: 0.9193\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2077 - accuracy: 0.9282 - val_loss: 0.2090 - val_accuracy: 0.9193\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.9255 - val_loss: 0.2079 - val_accuracy: 0.9193\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1827 - accuracy: 0.9310 - val_loss: 0.2064 - val_accuracy: 0.9193\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.9340 - val_loss: 0.2051 - val_accuracy: 0.9180\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1917 - accuracy: 0.9284 - val_loss: 0.2035 - val_accuracy: 0.9180\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2171 - accuracy: 0.9142 - val_loss: 0.2023 - val_accuracy: 0.9180\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1948 - accuracy: 0.9318 - val_loss: 0.2002 - val_accuracy: 0.9193\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1999 - accuracy: 0.9122 - val_loss: 0.1986 - val_accuracy: 0.9193\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9124 - val_loss: 0.1967 - val_accuracy: 0.9193\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9154 - val_loss: 0.1955 - val_accuracy: 0.9180\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1923 - accuracy: 0.9195 - val_loss: 0.1939 - val_accuracy: 0.9167\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1746 - accuracy: 0.9300 - val_loss: 0.1911 - val_accuracy: 0.9167\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2220 - accuracy: 0.9020 - val_loss: 0.1888 - val_accuracy: 0.9180\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1888 - accuracy: 0.9109 - val_loss: 0.1875 - val_accuracy: 0.9193\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1952 - accuracy: 0.9139 - val_loss: 0.1847 - val_accuracy: 0.9193\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1836 - accuracy: 0.9208 - val_loss: 0.1830 - val_accuracy: 0.9193\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9098 - val_loss: 0.1805 - val_accuracy: 0.9193\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1748 - accuracy: 0.9288 - val_loss: 0.1786 - val_accuracy: 0.9193\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1806 - accuracy: 0.9195 - val_loss: 0.1766 - val_accuracy: 0.9206\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1929 - accuracy: 0.9103 - val_loss: 0.1740 - val_accuracy: 0.9220\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1756 - accuracy: 0.9296 - val_loss: 0.1727 - val_accuracy: 0.9259\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1730 - accuracy: 0.9398 - val_loss: 0.1701 - val_accuracy: 0.9272\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1655 - accuracy: 0.9312 - val_loss: 0.1677 - val_accuracy: 0.9233\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1616 - accuracy: 0.9311 - val_loss: 0.1656 - val_accuracy: 0.9272\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.9315 - val_loss: 0.1638 - val_accuracy: 0.9272\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1690 - accuracy: 0.9171 - val_loss: 0.1623 - val_accuracy: 0.9272\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1768 - accuracy: 0.9174 - val_loss: 0.1606 - val_accuracy: 0.9286\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.9415 - val_loss: 0.1588 - val_accuracy: 0.9299\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1950 - accuracy: 0.9209 - val_loss: 0.1582 - val_accuracy: 0.9312\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1495 - accuracy: 0.9365 - val_loss: 0.1569 - val_accuracy: 0.9378\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1607 - accuracy: 0.9329 - val_loss: 0.1539 - val_accuracy: 0.9325\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1482 - accuracy: 0.9353 - val_loss: 0.1527 - val_accuracy: 0.9325\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1801 - accuracy: 0.9203 - val_loss: 0.1519 - val_accuracy: 0.9325\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1456 - accuracy: 0.9338 - val_loss: 0.1514 - val_accuracy: 0.9339\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1490 - accuracy: 0.9455 - val_loss: 0.1493 - val_accuracy: 0.9378\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1467 - accuracy: 0.9399 - val_loss: 0.1475 - val_accuracy: 0.9339\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1377 - accuracy: 0.9439 - val_loss: 0.1473 - val_accuracy: 0.9365\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1605 - accuracy: 0.9366 - val_loss: 0.1455 - val_accuracy: 0.9378\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1434 - accuracy: 0.9415 - val_loss: 0.1452 - val_accuracy: 0.9365\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9446 - val_loss: 0.1432 - val_accuracy: 0.9431\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1410 - accuracy: 0.9397 - val_loss: 0.1426 - val_accuracy: 0.9405\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1439 - accuracy: 0.9324 - val_loss: 0.1408 - val_accuracy: 0.9378\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1420 - accuracy: 0.9410 - val_loss: 0.1403 - val_accuracy: 0.9339\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1213 - accuracy: 0.9520 - val_loss: 0.1387 - val_accuracy: 0.9405\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1350 - accuracy: 0.9434 - val_loss: 0.1379 - val_accuracy: 0.9405\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1515 - accuracy: 0.9317 - val_loss: 0.1365 - val_accuracy: 0.9418\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_8_input'), name='dense_8_input', description=\"created by layer 'dense_8_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "3\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 756, 4)            20        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 756, 20)           100       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 756, 20)           420       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 756, 1)            21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      " 1/38 [..............................] - ETA: 26s - loss: 0.6919 - accuracy: 0.5000WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 0.6637 - accuracy: 0.6760 - val_loss: 0.5800 - val_accuracy: 0.7910\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5508 - accuracy: 0.8091 - val_loss: 0.4586 - val_accuracy: 0.8426\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8716 - val_loss: 0.3615 - val_accuracy: 0.8915\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.9011 - val_loss: 0.3038 - val_accuracy: 0.9127\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.9078 - val_loss: 0.2788 - val_accuracy: 0.9233\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2636 - accuracy: 0.9261 - val_loss: 0.2660 - val_accuracy: 0.9180\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2680 - accuracy: 0.9173 - val_loss: 0.2586 - val_accuracy: 0.9193\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2742 - accuracy: 0.9197 - val_loss: 0.2528 - val_accuracy: 0.9193\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.9152 - val_loss: 0.2481 - val_accuracy: 0.9206\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.9299 - val_loss: 0.2451 - val_accuracy: 0.9206\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2397 - accuracy: 0.9173 - val_loss: 0.2417 - val_accuracy: 0.9206\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2040 - accuracy: 0.9324 - val_loss: 0.2391 - val_accuracy: 0.9206\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2693 - accuracy: 0.9160 - val_loss: 0.2369 - val_accuracy: 0.9206\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2081 - accuracy: 0.9211 - val_loss: 0.2346 - val_accuracy: 0.9220\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2222 - accuracy: 0.9245 - val_loss: 0.2330 - val_accuracy: 0.9220\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2601 - accuracy: 0.9252 - val_loss: 0.2311 - val_accuracy: 0.9220\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2262 - accuracy: 0.9217 - val_loss: 0.2290 - val_accuracy: 0.9193\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.9106 - val_loss: 0.2280 - val_accuracy: 0.9193\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2746 - accuracy: 0.9077 - val_loss: 0.2258 - val_accuracy: 0.9206\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2384 - accuracy: 0.9251 - val_loss: 0.2243 - val_accuracy: 0.9233\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2087 - accuracy: 0.9304 - val_loss: 0.2230 - val_accuracy: 0.9299\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2110 - accuracy: 0.9359 - val_loss: 0.2210 - val_accuracy: 0.9286\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.9260 - val_loss: 0.2205 - val_accuracy: 0.9259\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2014 - accuracy: 0.9250 - val_loss: 0.2182 - val_accuracy: 0.9325\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1991 - accuracy: 0.9476 - val_loss: 0.2167 - val_accuracy: 0.9339\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2214 - accuracy: 0.9352 - val_loss: 0.2148 - val_accuracy: 0.9325\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2432 - accuracy: 0.9265 - val_loss: 0.2133 - val_accuracy: 0.9365\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2047 - accuracy: 0.9318 - val_loss: 0.2119 - val_accuracy: 0.9339\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9429 - val_loss: 0.2102 - val_accuracy: 0.9365\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2870 - accuracy: 0.9184 - val_loss: 0.2089 - val_accuracy: 0.9392\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.9349 - val_loss: 0.2070 - val_accuracy: 0.9378\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2186 - accuracy: 0.9408 - val_loss: 0.2055 - val_accuracy: 0.9392\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1919 - accuracy: 0.9392 - val_loss: 0.2038 - val_accuracy: 0.9378\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9393 - val_loss: 0.2022 - val_accuracy: 0.9392\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2354 - accuracy: 0.9366 - val_loss: 0.2010 - val_accuracy: 0.9392\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9338 - val_loss: 0.1992 - val_accuracy: 0.9378\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2002 - accuracy: 0.9343 - val_loss: 0.1980 - val_accuracy: 0.9405\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9418 - val_loss: 0.1961 - val_accuracy: 0.9378\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9322 - val_loss: 0.1958 - val_accuracy: 0.9431\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9341 - val_loss: 0.1927 - val_accuracy: 0.9418\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1781 - accuracy: 0.9524 - val_loss: 0.1912 - val_accuracy: 0.9392\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9329 - val_loss: 0.1899 - val_accuracy: 0.9405\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1771 - accuracy: 0.9364 - val_loss: 0.1878 - val_accuracy: 0.9418\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1796 - accuracy: 0.9506 - val_loss: 0.1860 - val_accuracy: 0.9431\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2042 - accuracy: 0.9454 - val_loss: 0.1844 - val_accuracy: 0.9431\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.9366 - val_loss: 0.1826 - val_accuracy: 0.9444\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2023 - accuracy: 0.9227 - val_loss: 0.1811 - val_accuracy: 0.9444\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1987 - accuracy: 0.9387 - val_loss: 0.1783 - val_accuracy: 0.9458\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1763 - accuracy: 0.9475 - val_loss: 0.1760 - val_accuracy: 0.9484\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9387 - val_loss: 0.1744 - val_accuracy: 0.9484\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1738 - accuracy: 0.9434 - val_loss: 0.1708 - val_accuracy: 0.9484\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.9495 - val_loss: 0.1684 - val_accuracy: 0.9484\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1934 - accuracy: 0.9455 - val_loss: 0.1657 - val_accuracy: 0.9484\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1486 - accuracy: 0.9481 - val_loss: 0.1637 - val_accuracy: 0.9484\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1819 - accuracy: 0.9452 - val_loss: 0.1605 - val_accuracy: 0.9497\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1542 - accuracy: 0.9499 - val_loss: 0.1594 - val_accuracy: 0.9497\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1722 - accuracy: 0.9444 - val_loss: 0.1559 - val_accuracy: 0.9484\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9507 - val_loss: 0.1536 - val_accuracy: 0.9497\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1497 - accuracy: 0.9519 - val_loss: 0.1509 - val_accuracy: 0.9524\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1452 - accuracy: 0.9556 - val_loss: 0.1486 - val_accuracy: 0.9497\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1488 - accuracy: 0.9501 - val_loss: 0.1462 - val_accuracy: 0.9524\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1450 - accuracy: 0.9457 - val_loss: 0.1443 - val_accuracy: 0.9524\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1282 - accuracy: 0.9564 - val_loss: 0.1409 - val_accuracy: 0.9524\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1689 - accuracy: 0.9428 - val_loss: 0.1386 - val_accuracy: 0.9511\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.9353 - val_loss: 0.1371 - val_accuracy: 0.9511\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1626 - accuracy: 0.9455 - val_loss: 0.1340 - val_accuracy: 0.9563\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1478 - accuracy: 0.9419 - val_loss: 0.1319 - val_accuracy: 0.9524\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9496 - val_loss: 0.1293 - val_accuracy: 0.9563\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1121 - accuracy: 0.9527 - val_loss: 0.1279 - val_accuracy: 0.9563\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1201 - accuracy: 0.9580 - val_loss: 0.1260 - val_accuracy: 0.9577\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1282 - accuracy: 0.9575 - val_loss: 0.1219 - val_accuracy: 0.9590\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1498 - accuracy: 0.9526 - val_loss: 0.1208 - val_accuracy: 0.9563\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1263 - accuracy: 0.9612 - val_loss: 0.1172 - val_accuracy: 0.9603\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0999 - accuracy: 0.9648 - val_loss: 0.1149 - val_accuracy: 0.9603\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1177 - accuracy: 0.9636 - val_loss: 0.1126 - val_accuracy: 0.9616\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1041 - accuracy: 0.9623 - val_loss: 0.1102 - val_accuracy: 0.9643\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1092 - accuracy: 0.9598 - val_loss: 0.1097 - val_accuracy: 0.9630\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1339 - accuracy: 0.9589 - val_loss: 0.1073 - val_accuracy: 0.9643\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.9575 - val_loss: 0.1044 - val_accuracy: 0.9656\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.9681 - val_loss: 0.1032 - val_accuracy: 0.9669\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1037 - accuracy: 0.9606 - val_loss: 0.1049 - val_accuracy: 0.9630\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1106 - accuracy: 0.9538 - val_loss: 0.0992 - val_accuracy: 0.9656\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.9663 - val_loss: 0.0986 - val_accuracy: 0.9656\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1192 - accuracy: 0.9624 - val_loss: 0.0961 - val_accuracy: 0.9656\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0967 - accuracy: 0.9542 - val_loss: 0.0947 - val_accuracy: 0.9643\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9808 - val_loss: 0.0936 - val_accuracy: 0.9656\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0895 - accuracy: 0.9748 - val_loss: 0.0919 - val_accuracy: 0.9656\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0928 - accuracy: 0.9670 - val_loss: 0.0906 - val_accuracy: 0.9656\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0937 - accuracy: 0.9671 - val_loss: 0.0906 - val_accuracy: 0.9683\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1047 - accuracy: 0.9585 - val_loss: 0.0895 - val_accuracy: 0.9709\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0942 - accuracy: 0.9645 - val_loss: 0.0870 - val_accuracy: 0.9669\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0971 - accuracy: 0.9661 - val_loss: 0.0865 - val_accuracy: 0.9683\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9660 - val_loss: 0.0850 - val_accuracy: 0.9669\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0801 - accuracy: 0.9667 - val_loss: 0.0838 - val_accuracy: 0.9696\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1158 - accuracy: 0.9615 - val_loss: 0.0839 - val_accuracy: 0.9696\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0829 - accuracy: 0.9728 - val_loss: 0.0821 - val_accuracy: 0.9709\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0787 - accuracy: 0.9770 - val_loss: 0.0814 - val_accuracy: 0.9735\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.9739 - val_loss: 0.0818 - val_accuracy: 0.9696\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0686 - accuracy: 0.9761 - val_loss: 0.0804 - val_accuracy: 0.9735\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.9735 - val_loss: 0.0794 - val_accuracy: 0.9749\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "4\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 756, 4)            20        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 756, 20)           100       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 756, 20)           420       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 756, 1)            21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_16_input'), name='dense_16_input', description=\"created by layer 'dense_16_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_16_input'), name='dense_16_input', description=\"created by layer 'dense_16_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      " 1/38 [..............................] - ETA: 25s - loss: 0.6805 - accuracy: 0.6000WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_16_input'), name='dense_16_input', description=\"created by layer 'dense_16_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 0.6740 - accuracy: 0.6430 - val_loss: 0.6216 - val_accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5949 - accuracy: 0.6910 - val_loss: 0.5416 - val_accuracy: 0.6905\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7191 - val_loss: 0.4697 - val_accuracy: 0.8399\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.8595 - val_loss: 0.4170 - val_accuracy: 0.8585\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4129 - accuracy: 0.8588 - val_loss: 0.3850 - val_accuracy: 0.8664\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3614 - accuracy: 0.8807 - val_loss: 0.3631 - val_accuracy: 0.8743\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3537 - accuracy: 0.8856 - val_loss: 0.3439 - val_accuracy: 0.8796\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.8889 - val_loss: 0.3281 - val_accuracy: 0.8849\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2965 - accuracy: 0.9086 - val_loss: 0.3163 - val_accuracy: 0.8823\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3238 - accuracy: 0.8760 - val_loss: 0.3068 - val_accuracy: 0.8902\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3250 - accuracy: 0.8844 - val_loss: 0.2985 - val_accuracy: 0.8981\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3110 - accuracy: 0.8911 - val_loss: 0.2905 - val_accuracy: 0.9008\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.8904 - val_loss: 0.2840 - val_accuracy: 0.9061\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2481 - accuracy: 0.9213 - val_loss: 0.2779 - val_accuracy: 0.9021\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8888 - val_loss: 0.2728 - val_accuracy: 0.9074\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.9039 - val_loss: 0.2674 - val_accuracy: 0.9061\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2473 - accuracy: 0.9123 - val_loss: 0.2625 - val_accuracy: 0.9061\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2373 - accuracy: 0.9278 - val_loss: 0.2590 - val_accuracy: 0.9074\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.9069 - val_loss: 0.2541 - val_accuracy: 0.9140\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2816 - accuracy: 0.9064 - val_loss: 0.2498 - val_accuracy: 0.9153\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.9185 - val_loss: 0.2446 - val_accuracy: 0.9180\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2847 - accuracy: 0.8986 - val_loss: 0.2402 - val_accuracy: 0.9193\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.9252 - val_loss: 0.2367 - val_accuracy: 0.9220\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2529 - accuracy: 0.9030 - val_loss: 0.2326 - val_accuracy: 0.9220\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2075 - accuracy: 0.9352 - val_loss: 0.2278 - val_accuracy: 0.9286\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.9099 - val_loss: 0.2236 - val_accuracy: 0.9299\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2312 - accuracy: 0.9290 - val_loss: 0.2215 - val_accuracy: 0.9272\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2262 - accuracy: 0.9408 - val_loss: 0.2169 - val_accuracy: 0.9272\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2161 - accuracy: 0.9304 - val_loss: 0.2136 - val_accuracy: 0.9339\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2088 - accuracy: 0.9254 - val_loss: 0.2100 - val_accuracy: 0.9339\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2170 - accuracy: 0.9255 - val_loss: 0.2060 - val_accuracy: 0.9299\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2088 - accuracy: 0.9240 - val_loss: 0.2032 - val_accuracy: 0.9365\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2198 - accuracy: 0.9306 - val_loss: 0.2003 - val_accuracy: 0.9286\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9374 - val_loss: 0.1970 - val_accuracy: 0.9352\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2245 - accuracy: 0.9273 - val_loss: 0.1942 - val_accuracy: 0.9286\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9277 - val_loss: 0.1911 - val_accuracy: 0.9352\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1596 - accuracy: 0.9504 - val_loss: 0.1887 - val_accuracy: 0.9352\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2162 - accuracy: 0.9270 - val_loss: 0.1875 - val_accuracy: 0.9339\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1890 - accuracy: 0.9263 - val_loss: 0.1828 - val_accuracy: 0.9365\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1793 - accuracy: 0.9343 - val_loss: 0.1809 - val_accuracy: 0.9365\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1687 - accuracy: 0.9379 - val_loss: 0.1776 - val_accuracy: 0.9365\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1742 - accuracy: 0.9363 - val_loss: 0.1763 - val_accuracy: 0.9378\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1707 - accuracy: 0.9313 - val_loss: 0.1721 - val_accuracy: 0.9378\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1513 - accuracy: 0.9511 - val_loss: 0.1693 - val_accuracy: 0.9405\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1785 - accuracy: 0.9358 - val_loss: 0.1667 - val_accuracy: 0.9405\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1617 - accuracy: 0.9462 - val_loss: 0.1646 - val_accuracy: 0.9392\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1570 - accuracy: 0.9378 - val_loss: 0.1612 - val_accuracy: 0.9418\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1667 - accuracy: 0.9380 - val_loss: 0.1581 - val_accuracy: 0.9418\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1576 - accuracy: 0.9466 - val_loss: 0.1556 - val_accuracy: 0.9431\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1799 - accuracy: 0.9407 - val_loss: 0.1530 - val_accuracy: 0.9431\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1439 - accuracy: 0.9479 - val_loss: 0.1515 - val_accuracy: 0.9458\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1669 - accuracy: 0.9365 - val_loss: 0.1483 - val_accuracy: 0.9444\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1672 - accuracy: 0.9317 - val_loss: 0.1465 - val_accuracy: 0.9458\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9524 - val_loss: 0.1443 - val_accuracy: 0.9444\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1611 - accuracy: 0.9455 - val_loss: 0.1412 - val_accuracy: 0.9471\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9391 - val_loss: 0.1395 - val_accuracy: 0.9524\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1403 - accuracy: 0.9476 - val_loss: 0.1447 - val_accuracy: 0.9444\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1359 - accuracy: 0.9399 - val_loss: 0.1334 - val_accuracy: 0.9511\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1235 - accuracy: 0.9539 - val_loss: 0.1311 - val_accuracy: 0.9511\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1264 - accuracy: 0.9544 - val_loss: 0.1288 - val_accuracy: 0.9537\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1115 - accuracy: 0.9634 - val_loss: 0.1273 - val_accuracy: 0.9550\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1048 - accuracy: 0.9672 - val_loss: 0.1251 - val_accuracy: 0.9550\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1500 - accuracy: 0.9368 - val_loss: 0.1249 - val_accuracy: 0.9550\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9582 - val_loss: 0.1324 - val_accuracy: 0.9550\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1337 - accuracy: 0.9554 - val_loss: 0.1185 - val_accuracy: 0.9590\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1303 - accuracy: 0.9507 - val_loss: 0.1195 - val_accuracy: 0.9577\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1080 - accuracy: 0.9665 - val_loss: 0.1149 - val_accuracy: 0.9616\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1386 - accuracy: 0.9419 - val_loss: 0.1130 - val_accuracy: 0.9616\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1053 - accuracy: 0.9641 - val_loss: 0.1115 - val_accuracy: 0.9630\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.9719 - val_loss: 0.1112 - val_accuracy: 0.9577\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1340 - accuracy: 0.9592 - val_loss: 0.1091 - val_accuracy: 0.9669\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1120 - accuracy: 0.9633 - val_loss: 0.1092 - val_accuracy: 0.9616\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1184 - accuracy: 0.9516 - val_loss: 0.1066 - val_accuracy: 0.9630\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0901 - accuracy: 0.9696 - val_loss: 0.1144 - val_accuracy: 0.9656\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1328 - accuracy: 0.9561 - val_loss: 0.1034 - val_accuracy: 0.9656\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0768 - accuracy: 0.9817 - val_loss: 0.1025 - val_accuracy: 0.9656\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9744 - val_loss: 0.1015 - val_accuracy: 0.9669\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1017 - accuracy: 0.9597 - val_loss: 0.0993 - val_accuracy: 0.9683\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 0.9660 - val_loss: 0.0978 - val_accuracy: 0.9683\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0973 - accuracy: 0.9681 - val_loss: 0.1006 - val_accuracy: 0.9683\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0909 - accuracy: 0.9756 - val_loss: 0.0956 - val_accuracy: 0.9656\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1120 - accuracy: 0.9722 - val_loss: 0.0955 - val_accuracy: 0.9696\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0965 - accuracy: 0.9765 - val_loss: 0.0926 - val_accuracy: 0.9709\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1066 - accuracy: 0.9659 - val_loss: 0.0945 - val_accuracy: 0.9683\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0897 - accuracy: 0.9698 - val_loss: 0.0907 - val_accuracy: 0.9722\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9722 - val_loss: 0.0899 - val_accuracy: 0.9722\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1049 - accuracy: 0.9614 - val_loss: 0.0890 - val_accuracy: 0.9722\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.9662 - val_loss: 0.0921 - val_accuracy: 0.9696\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 0.9828 - val_loss: 0.0886 - val_accuracy: 0.9709\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0952 - accuracy: 0.9686 - val_loss: 0.0863 - val_accuracy: 0.9722\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0949 - accuracy: 0.9654 - val_loss: 0.0902 - val_accuracy: 0.9709\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9791 - val_loss: 0.0842 - val_accuracy: 0.9722\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 0.9749 - val_loss: 0.0836 - val_accuracy: 0.9735\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0845 - accuracy: 0.9687 - val_loss: 0.0837 - val_accuracy: 0.9735\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9726 - val_loss: 0.0826 - val_accuracy: 0.9722\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9792 - val_loss: 0.0818 - val_accuracy: 0.9722\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0881 - accuracy: 0.9685 - val_loss: 0.0836 - val_accuracy: 0.9722\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9648 - val_loss: 0.0822 - val_accuracy: 0.9709\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1011 - accuracy: 0.9574 - val_loss: 0.0830 - val_accuracy: 0.9735\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0919 - accuracy: 0.9711 - val_loss: 0.0808 - val_accuracy: 0.9709\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_16_input'), name='dense_16_input', description=\"created by layer 'dense_16_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "5\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 756, 4)            20        \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 756, 20)           100       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 756, 20)           420       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 756, 1)            21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_20_input'), name='dense_20_input', description=\"created by layer 'dense_20_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_20_input'), name='dense_20_input', description=\"created by layer 'dense_20_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      " 1/38 [..............................] - ETA: 24s - loss: 0.7144 - accuracy: 0.3500WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_20_input'), name='dense_20_input', description=\"created by layer 'dense_20_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 0.6849 - accuracy: 0.6652 - val_loss: 0.6227 - val_accuracy: 0.8161\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6043 - accuracy: 0.8056 - val_loss: 0.5266 - val_accuracy: 0.8201\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.8424 - val_loss: 0.4328 - val_accuracy: 0.8373\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4084 - accuracy: 0.8501 - val_loss: 0.3796 - val_accuracy: 0.8479\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3664 - accuracy: 0.8574 - val_loss: 0.3481 - val_accuracy: 0.8796\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3200 - accuracy: 0.8899 - val_loss: 0.3263 - val_accuracy: 0.8902\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3547 - accuracy: 0.8831 - val_loss: 0.3119 - val_accuracy: 0.8929\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2933 - accuracy: 0.9070 - val_loss: 0.3018 - val_accuracy: 0.8929\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2960 - accuracy: 0.8841 - val_loss: 0.2935 - val_accuracy: 0.8942\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2923 - accuracy: 0.8926 - val_loss: 0.2856 - val_accuracy: 0.8955\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2809 - accuracy: 0.8917 - val_loss: 0.2799 - val_accuracy: 0.8915\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.9082 - val_loss: 0.2747 - val_accuracy: 0.8968\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2839 - accuracy: 0.8990 - val_loss: 0.2703 - val_accuracy: 0.9048\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2522 - accuracy: 0.9212 - val_loss: 0.2662 - val_accuracy: 0.9048\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2574 - accuracy: 0.9073 - val_loss: 0.2628 - val_accuracy: 0.9034\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2486 - accuracy: 0.9107 - val_loss: 0.2590 - val_accuracy: 0.9061\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2486 - accuracy: 0.9022 - val_loss: 0.2559 - val_accuracy: 0.9087\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2387 - accuracy: 0.9201 - val_loss: 0.2530 - val_accuracy: 0.9074\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2709 - accuracy: 0.8903 - val_loss: 0.2502 - val_accuracy: 0.9140\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2628 - accuracy: 0.9204 - val_loss: 0.2477 - val_accuracy: 0.9114\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.9077 - val_loss: 0.2453 - val_accuracy: 0.9180\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2489 - accuracy: 0.8990 - val_loss: 0.2428 - val_accuracy: 0.9153\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2194 - accuracy: 0.9344 - val_loss: 0.2406 - val_accuracy: 0.9180\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2122 - accuracy: 0.9338 - val_loss: 0.2390 - val_accuracy: 0.9206\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2329 - accuracy: 0.9235 - val_loss: 0.2371 - val_accuracy: 0.9167\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2527 - accuracy: 0.9139 - val_loss: 0.2355 - val_accuracy: 0.9180\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2322 - accuracy: 0.9102 - val_loss: 0.2341 - val_accuracy: 0.9206\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2623 - accuracy: 0.9142 - val_loss: 0.2326 - val_accuracy: 0.9193\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2584 - accuracy: 0.8959 - val_loss: 0.2312 - val_accuracy: 0.9206\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2517 - accuracy: 0.9038 - val_loss: 0.2300 - val_accuracy: 0.9193\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2101 - accuracy: 0.9351 - val_loss: 0.2286 - val_accuracy: 0.9193\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2063 - accuracy: 0.9270 - val_loss: 0.2275 - val_accuracy: 0.9193\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2319 - accuracy: 0.9078 - val_loss: 0.2266 - val_accuracy: 0.9206\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2241 - accuracy: 0.9282 - val_loss: 0.2250 - val_accuracy: 0.9206\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.9183 - val_loss: 0.2240 - val_accuracy: 0.9233\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.9224 - val_loss: 0.2225 - val_accuracy: 0.9233\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2397 - accuracy: 0.9104 - val_loss: 0.2212 - val_accuracy: 0.9233\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2519 - accuracy: 0.9272 - val_loss: 0.2200 - val_accuracy: 0.9246\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9247 - val_loss: 0.2192 - val_accuracy: 0.9233\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.9201 - val_loss: 0.2179 - val_accuracy: 0.9246\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1935 - accuracy: 0.9351 - val_loss: 0.2167 - val_accuracy: 0.9259\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2251 - accuracy: 0.9223 - val_loss: 0.2158 - val_accuracy: 0.9259\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2444 - accuracy: 0.9256 - val_loss: 0.2151 - val_accuracy: 0.9272\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2133 - accuracy: 0.9347 - val_loss: 0.2142 - val_accuracy: 0.9259\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2149 - accuracy: 0.9298 - val_loss: 0.2133 - val_accuracy: 0.9272\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.9254 - val_loss: 0.2125 - val_accuracy: 0.9286\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2357 - accuracy: 0.9317 - val_loss: 0.2116 - val_accuracy: 0.9286\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9335 - val_loss: 0.2108 - val_accuracy: 0.9286\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2330 - accuracy: 0.9163 - val_loss: 0.2099 - val_accuracy: 0.9272\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2382 - accuracy: 0.9189 - val_loss: 0.2093 - val_accuracy: 0.9286\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2019 - accuracy: 0.9289 - val_loss: 0.2083 - val_accuracy: 0.9286\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2318 - accuracy: 0.9200 - val_loss: 0.2077 - val_accuracy: 0.9286\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1866 - accuracy: 0.9371 - val_loss: 0.2069 - val_accuracy: 0.9272\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2075 - accuracy: 0.9251 - val_loss: 0.2064 - val_accuracy: 0.9272\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2147 - accuracy: 0.9270 - val_loss: 0.2055 - val_accuracy: 0.9272\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.9252 - val_loss: 0.2049 - val_accuracy: 0.9286\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9145 - val_loss: 0.2042 - val_accuracy: 0.9286\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1899 - accuracy: 0.9250 - val_loss: 0.2036 - val_accuracy: 0.9272\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2320 - accuracy: 0.9070 - val_loss: 0.2031 - val_accuracy: 0.9286\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2175 - accuracy: 0.9321 - val_loss: 0.2023 - val_accuracy: 0.9286\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2103 - accuracy: 0.9124 - val_loss: 0.2022 - val_accuracy: 0.9299\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.9196 - val_loss: 0.2013 - val_accuracy: 0.9286\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.9218 - val_loss: 0.2005 - val_accuracy: 0.9272\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1877 - accuracy: 0.9266 - val_loss: 0.2000 - val_accuracy: 0.9312\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9281 - val_loss: 0.1994 - val_accuracy: 0.9299\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1970 - accuracy: 0.9315 - val_loss: 0.1987 - val_accuracy: 0.9325\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2032 - accuracy: 0.9287 - val_loss: 0.1981 - val_accuracy: 0.9312\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1914 - accuracy: 0.9383 - val_loss: 0.1973 - val_accuracy: 0.9299\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2012 - accuracy: 0.9300 - val_loss: 0.1970 - val_accuracy: 0.9312\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1793 - accuracy: 0.9415 - val_loss: 0.1963 - val_accuracy: 0.9339\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1752 - accuracy: 0.9392 - val_loss: 0.1965 - val_accuracy: 0.9312\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1893 - accuracy: 0.9352 - val_loss: 0.1950 - val_accuracy: 0.9339\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1794 - accuracy: 0.9308 - val_loss: 0.1946 - val_accuracy: 0.9312\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1671 - accuracy: 0.9427 - val_loss: 0.1941 - val_accuracy: 0.9312\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9334 - val_loss: 0.1936 - val_accuracy: 0.9339\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2202 - accuracy: 0.9163 - val_loss: 0.1927 - val_accuracy: 0.9312\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2226 - accuracy: 0.9182 - val_loss: 0.1922 - val_accuracy: 0.9312\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2051 - accuracy: 0.9335 - val_loss: 0.1915 - val_accuracy: 0.9312\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2053 - accuracy: 0.9246 - val_loss: 0.1909 - val_accuracy: 0.9312\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1906 - accuracy: 0.9286 - val_loss: 0.1903 - val_accuracy: 0.9339\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9319 - val_loss: 0.1902 - val_accuracy: 0.9339\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1978 - accuracy: 0.9300 - val_loss: 0.1894 - val_accuracy: 0.9312\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1881 - accuracy: 0.9233 - val_loss: 0.1887 - val_accuracy: 0.9312\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2052 - accuracy: 0.9208 - val_loss: 0.1883 - val_accuracy: 0.9339\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1700 - accuracy: 0.9422 - val_loss: 0.1876 - val_accuracy: 0.9325\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1897 - accuracy: 0.9337 - val_loss: 0.1870 - val_accuracy: 0.9339\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1906 - accuracy: 0.9377 - val_loss: 0.1864 - val_accuracy: 0.9325\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9402 - val_loss: 0.1861 - val_accuracy: 0.9325\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1970 - accuracy: 0.9241 - val_loss: 0.1853 - val_accuracy: 0.9325\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1865 - accuracy: 0.9322 - val_loss: 0.1849 - val_accuracy: 0.9325\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1974 - accuracy: 0.9320 - val_loss: 0.1843 - val_accuracy: 0.9325\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1511 - accuracy: 0.9414 - val_loss: 0.1838 - val_accuracy: 0.9325\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1771 - accuracy: 0.9422 - val_loss: 0.1833 - val_accuracy: 0.9325\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1720 - accuracy: 0.9319 - val_loss: 0.1828 - val_accuracy: 0.9325\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1566 - accuracy: 0.9439 - val_loss: 0.1821 - val_accuracy: 0.9325\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1632 - accuracy: 0.9443 - val_loss: 0.1817 - val_accuracy: 0.9325\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1656 - accuracy: 0.9346 - val_loss: 0.1812 - val_accuracy: 0.9325\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9337 - val_loss: 0.1807 - val_accuracy: 0.9325\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1660 - accuracy: 0.9430 - val_loss: 0.1804 - val_accuracy: 0.9339\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1583 - accuracy: 0.9481 - val_loss: 0.1798 - val_accuracy: 0.9325\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_20_input'), name='dense_20_input', description=\"created by layer 'dense_20_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "6\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 756, 4)            20        \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 756, 20)           100       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 756, 20)           420       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 756, 1)            21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_24_input'), name='dense_24_input', description=\"created by layer 'dense_24_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_24_input'), name='dense_24_input', description=\"created by layer 'dense_24_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      " 1/38 [..............................] - ETA: 25s - loss: 0.7352 - accuracy: 0.6000WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_24_input'), name='dense_24_input', description=\"created by layer 'dense_24_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 0.6940 - accuracy: 0.6566 - val_loss: 0.5932 - val_accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5795 - accuracy: 0.6566 - val_loss: 0.4857 - val_accuracy: 0.8056\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4683 - accuracy: 0.7943 - val_loss: 0.4061 - val_accuracy: 0.8505\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3911 - accuracy: 0.8344 - val_loss: 0.3451 - val_accuracy: 0.8664\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.8790 - val_loss: 0.3076 - val_accuracy: 0.8730\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2911 - accuracy: 0.8753 - val_loss: 0.2857 - val_accuracy: 0.8796\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8884 - val_loss: 0.2734 - val_accuracy: 0.8981\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2673 - accuracy: 0.9066 - val_loss: 0.2655 - val_accuracy: 0.9048\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2563 - accuracy: 0.9131 - val_loss: 0.2591 - val_accuracy: 0.9048\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2276 - accuracy: 0.9241 - val_loss: 0.2556 - val_accuracy: 0.9074\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2537 - accuracy: 0.9084 - val_loss: 0.2527 - val_accuracy: 0.9087\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2439 - accuracy: 0.9220 - val_loss: 0.2500 - val_accuracy: 0.9127\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2661 - accuracy: 0.8900 - val_loss: 0.2476 - val_accuracy: 0.9153\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2728 - accuracy: 0.8957 - val_loss: 0.2459 - val_accuracy: 0.9140\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9143 - val_loss: 0.2436 - val_accuracy: 0.9153\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.9096 - val_loss: 0.2421 - val_accuracy: 0.9140\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2426 - accuracy: 0.9004 - val_loss: 0.2399 - val_accuracy: 0.9127\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2368 - accuracy: 0.9195 - val_loss: 0.2382 - val_accuracy: 0.9127\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2177 - accuracy: 0.9165 - val_loss: 0.2366 - val_accuracy: 0.9114\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.8823 - val_loss: 0.2352 - val_accuracy: 0.9127\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2864 - accuracy: 0.8885 - val_loss: 0.2342 - val_accuracy: 0.9101\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2333 - accuracy: 0.9203 - val_loss: 0.2322 - val_accuracy: 0.9127\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2843 - accuracy: 0.8846 - val_loss: 0.2309 - val_accuracy: 0.9127\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2518 - accuracy: 0.9027 - val_loss: 0.2288 - val_accuracy: 0.9140\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.9075 - val_loss: 0.2275 - val_accuracy: 0.9140\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9134 - val_loss: 0.2262 - val_accuracy: 0.9180\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2231 - accuracy: 0.9212 - val_loss: 0.2253 - val_accuracy: 0.9140\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.9089 - val_loss: 0.2246 - val_accuracy: 0.9180\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9070 - val_loss: 0.2238 - val_accuracy: 0.9153\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1894 - accuracy: 0.9323 - val_loss: 0.2231 - val_accuracy: 0.9153\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2312 - accuracy: 0.8954 - val_loss: 0.2224 - val_accuracy: 0.9153\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2192 - accuracy: 0.9229 - val_loss: 0.2218 - val_accuracy: 0.9153\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2631 - accuracy: 0.9013 - val_loss: 0.2212 - val_accuracy: 0.9167\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1963 - accuracy: 0.9202 - val_loss: 0.2204 - val_accuracy: 0.9140\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2629 - accuracy: 0.9177 - val_loss: 0.2197 - val_accuracy: 0.9167\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.8974 - val_loss: 0.2190 - val_accuracy: 0.9167\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2622 - accuracy: 0.9169 - val_loss: 0.2182 - val_accuracy: 0.9167\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2150 - accuracy: 0.9104 - val_loss: 0.2176 - val_accuracy: 0.9180\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2172 - accuracy: 0.9173 - val_loss: 0.2167 - val_accuracy: 0.9153\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1762 - accuracy: 0.9430 - val_loss: 0.2161 - val_accuracy: 0.9153\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9170 - val_loss: 0.2151 - val_accuracy: 0.9180\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2218 - accuracy: 0.9291 - val_loss: 0.2142 - val_accuracy: 0.9193\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1991 - accuracy: 0.9183 - val_loss: 0.2132 - val_accuracy: 0.9193\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1915 - accuracy: 0.9245 - val_loss: 0.2120 - val_accuracy: 0.9193\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2335 - accuracy: 0.9007 - val_loss: 0.2109 - val_accuracy: 0.9153\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1916 - accuracy: 0.9306 - val_loss: 0.2096 - val_accuracy: 0.9193\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2093 - accuracy: 0.9269 - val_loss: 0.2080 - val_accuracy: 0.9193\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2076 - accuracy: 0.9249 - val_loss: 0.2064 - val_accuracy: 0.9233\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1901 - accuracy: 0.9228 - val_loss: 0.2053 - val_accuracy: 0.9220\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2233 - accuracy: 0.9226 - val_loss: 0.2037 - val_accuracy: 0.9233\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2082 - accuracy: 0.9223 - val_loss: 0.2014 - val_accuracy: 0.9206\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9361 - val_loss: 0.1999 - val_accuracy: 0.9233\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2168 - accuracy: 0.9160 - val_loss: 0.1977 - val_accuracy: 0.9233\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2155 - accuracy: 0.9210 - val_loss: 0.1959 - val_accuracy: 0.9259\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9256 - val_loss: 0.1943 - val_accuracy: 0.9272\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2051 - accuracy: 0.9183 - val_loss: 0.1922 - val_accuracy: 0.9299\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2129 - accuracy: 0.9147 - val_loss: 0.1903 - val_accuracy: 0.9299\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1704 - accuracy: 0.9386 - val_loss: 0.1885 - val_accuracy: 0.9312\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2081 - accuracy: 0.9227 - val_loss: 0.1869 - val_accuracy: 0.9312\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1756 - accuracy: 0.9311 - val_loss: 0.1843 - val_accuracy: 0.9325\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9374 - val_loss: 0.1818 - val_accuracy: 0.9352\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1871 - accuracy: 0.9348 - val_loss: 0.1796 - val_accuracy: 0.9352\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1873 - accuracy: 0.9329 - val_loss: 0.1773 - val_accuracy: 0.9352\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1919 - accuracy: 0.9184 - val_loss: 0.1746 - val_accuracy: 0.9365\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1759 - accuracy: 0.9411 - val_loss: 0.1722 - val_accuracy: 0.9405\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9390 - val_loss: 0.1693 - val_accuracy: 0.9418\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1541 - accuracy: 0.9464 - val_loss: 0.1671 - val_accuracy: 0.9458\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1760 - accuracy: 0.9400 - val_loss: 0.1647 - val_accuracy: 0.9471\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1735 - accuracy: 0.9430 - val_loss: 0.1627 - val_accuracy: 0.9471\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1899 - accuracy: 0.9253 - val_loss: 0.1599 - val_accuracy: 0.9484\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1383 - accuracy: 0.9569 - val_loss: 0.1596 - val_accuracy: 0.9484\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1407 - accuracy: 0.9555 - val_loss: 0.1552 - val_accuracy: 0.9497\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.9489 - val_loss: 0.1546 - val_accuracy: 0.9497\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2031 - accuracy: 0.9258 - val_loss: 0.1513 - val_accuracy: 0.9511\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9474 - val_loss: 0.1492 - val_accuracy: 0.9524\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1463 - accuracy: 0.9572 - val_loss: 0.1465 - val_accuracy: 0.9537\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1631 - accuracy: 0.9424 - val_loss: 0.1444 - val_accuracy: 0.9537\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.9561 - val_loss: 0.1417 - val_accuracy: 0.9550\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1396 - accuracy: 0.9566 - val_loss: 0.1394 - val_accuracy: 0.9550\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1338 - accuracy: 0.9573 - val_loss: 0.1368 - val_accuracy: 0.9563\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1448 - accuracy: 0.9601 - val_loss: 0.1349 - val_accuracy: 0.9577\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1551 - accuracy: 0.9374 - val_loss: 0.1324 - val_accuracy: 0.9590\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9617 - val_loss: 0.1302 - val_accuracy: 0.9590\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1239 - accuracy: 0.9621 - val_loss: 0.1285 - val_accuracy: 0.9590\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1392 - accuracy: 0.9569 - val_loss: 0.1273 - val_accuracy: 0.9603\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1320 - accuracy: 0.9649 - val_loss: 0.1253 - val_accuracy: 0.9603\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1515 - accuracy: 0.9452 - val_loss: 0.1226 - val_accuracy: 0.9616\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.9640 - val_loss: 0.1213 - val_accuracy: 0.9616\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1362 - accuracy: 0.9575 - val_loss: 0.1204 - val_accuracy: 0.9616\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.9705 - val_loss: 0.1194 - val_accuracy: 0.9616\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.9533 - val_loss: 0.1171 - val_accuracy: 0.9616\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1229 - accuracy: 0.9617 - val_loss: 0.1153 - val_accuracy: 0.9616\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1333 - accuracy: 0.9560 - val_loss: 0.1141 - val_accuracy: 0.9616\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9500 - val_loss: 0.1131 - val_accuracy: 0.9616\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1107 - accuracy: 0.9591 - val_loss: 0.1119 - val_accuracy: 0.9616\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1179 - accuracy: 0.9528 - val_loss: 0.1105 - val_accuracy: 0.9616\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1092 - accuracy: 0.9626 - val_loss: 0.1096 - val_accuracy: 0.9616\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1239 - accuracy: 0.9515 - val_loss: 0.1090 - val_accuracy: 0.9616\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1413 - accuracy: 0.9510 - val_loss: 0.1070 - val_accuracy: 0.9616\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1349 - accuracy: 0.9536 - val_loss: 0.1067 - val_accuracy: 0.9616\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_24_input'), name='dense_24_input', description=\"created by layer 'dense_24_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "7\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 756, 4)            20        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 756, 20)           100       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 756, 20)           420       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 756, 1)            21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_28_input'), name='dense_28_input', description=\"created by layer 'dense_28_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_28_input'), name='dense_28_input', description=\"created by layer 'dense_28_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      " 1/38 [..............................] - ETA: 25s - loss: 0.7047 - accuracy: 0.4000WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_28_input'), name='dense_28_input', description=\"created by layer 'dense_28_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "38/38 [==============================] - 1s 8ms/step - loss: 0.6654 - accuracy: 0.6372 - val_loss: 0.5684 - val_accuracy: 0.8228\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5477 - accuracy: 0.8183 - val_loss: 0.4669 - val_accuracy: 0.8638\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4432 - accuracy: 0.8701 - val_loss: 0.3758 - val_accuracy: 0.8783\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3565 - accuracy: 0.8820 - val_loss: 0.3157 - val_accuracy: 0.8810\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3061 - accuracy: 0.8915 - val_loss: 0.2863 - val_accuracy: 0.8849\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2838 - accuracy: 0.8953 - val_loss: 0.2689 - val_accuracy: 0.8955\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2825 - accuracy: 0.9006 - val_loss: 0.2597 - val_accuracy: 0.9114\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2763 - accuracy: 0.9155 - val_loss: 0.2519 - val_accuracy: 0.9193\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2557 - accuracy: 0.9225 - val_loss: 0.2470 - val_accuracy: 0.9193\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.9165 - val_loss: 0.2442 - val_accuracy: 0.9180\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9390 - val_loss: 0.2418 - val_accuracy: 0.9180\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.9083 - val_loss: 0.2400 - val_accuracy: 0.9180\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2306 - accuracy: 0.9240 - val_loss: 0.2386 - val_accuracy: 0.9180\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2107 - accuracy: 0.9362 - val_loss: 0.2375 - val_accuracy: 0.9193\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2329 - accuracy: 0.9301 - val_loss: 0.2365 - val_accuracy: 0.9193\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2388 - accuracy: 0.9181 - val_loss: 0.2352 - val_accuracy: 0.9193\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2359 - accuracy: 0.9196 - val_loss: 0.2343 - val_accuracy: 0.9180\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.9106 - val_loss: 0.2334 - val_accuracy: 0.9180\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2491 - accuracy: 0.9157 - val_loss: 0.2326 - val_accuracy: 0.9180\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2480 - accuracy: 0.9163 - val_loss: 0.2321 - val_accuracy: 0.9180\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.9280 - val_loss: 0.2309 - val_accuracy: 0.9180\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2830 - accuracy: 0.9029 - val_loss: 0.2302 - val_accuracy: 0.9193\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2156 - accuracy: 0.9215 - val_loss: 0.2294 - val_accuracy: 0.9193\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2337 - accuracy: 0.9166 - val_loss: 0.2289 - val_accuracy: 0.9220\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.9174 - val_loss: 0.2280 - val_accuracy: 0.9193\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2244 - accuracy: 0.9216 - val_loss: 0.2272 - val_accuracy: 0.9206\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2098 - accuracy: 0.9337 - val_loss: 0.2266 - val_accuracy: 0.9206\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2134 - accuracy: 0.9293 - val_loss: 0.2259 - val_accuracy: 0.9220\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 0.2245 - accuracy: 0.9202 - val_loss: 0.2252 - val_accuracy: 0.9220\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1990 - accuracy: 0.9330 - val_loss: 0.2247 - val_accuracy: 0.9246\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2037 - accuracy: 0.9291 - val_loss: 0.2238 - val_accuracy: 0.9246\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.9131 - val_loss: 0.2230 - val_accuracy: 0.9246\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1965 - accuracy: 0.9363 - val_loss: 0.2225 - val_accuracy: 0.9259\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2367 - accuracy: 0.9298 - val_loss: 0.2217 - val_accuracy: 0.9246\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2361 - accuracy: 0.9023 - val_loss: 0.2210 - val_accuracy: 0.9259\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9178 - val_loss: 0.2203 - val_accuracy: 0.9259\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2145 - accuracy: 0.9313 - val_loss: 0.2195 - val_accuracy: 0.9259\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2014 - accuracy: 0.9274 - val_loss: 0.2189 - val_accuracy: 0.9259\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2310 - accuracy: 0.9223 - val_loss: 0.2183 - val_accuracy: 0.9259\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2108 - accuracy: 0.9351 - val_loss: 0.2176 - val_accuracy: 0.9259\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1985 - accuracy: 0.9320 - val_loss: 0.2171 - val_accuracy: 0.9259\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2226 - accuracy: 0.9336 - val_loss: 0.2163 - val_accuracy: 0.9272\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1992 - accuracy: 0.9378 - val_loss: 0.2155 - val_accuracy: 0.9259\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2356 - accuracy: 0.9232 - val_loss: 0.2149 - val_accuracy: 0.9272\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2382 - accuracy: 0.9195 - val_loss: 0.2139 - val_accuracy: 0.9272\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.9060 - val_loss: 0.2137 - val_accuracy: 0.9272\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.9137 - val_loss: 0.2128 - val_accuracy: 0.9272\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2632 - accuracy: 0.9094 - val_loss: 0.2122 - val_accuracy: 0.9272\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2276 - accuracy: 0.9242 - val_loss: 0.2112 - val_accuracy: 0.9286\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9165 - val_loss: 0.2104 - val_accuracy: 0.9272\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2097 - accuracy: 0.9282 - val_loss: 0.2096 - val_accuracy: 0.9286\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1998 - accuracy: 0.9308 - val_loss: 0.2088 - val_accuracy: 0.9286\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2071 - accuracy: 0.9262 - val_loss: 0.2081 - val_accuracy: 0.9272\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1971 - accuracy: 0.9329 - val_loss: 0.2072 - val_accuracy: 0.9299\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2148 - accuracy: 0.9214 - val_loss: 0.2064 - val_accuracy: 0.9272\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2036 - accuracy: 0.9317 - val_loss: 0.2056 - val_accuracy: 0.9272\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1683 - accuracy: 0.9447 - val_loss: 0.2045 - val_accuracy: 0.9299\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2093 - accuracy: 0.9256 - val_loss: 0.2039 - val_accuracy: 0.9272\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2310 - accuracy: 0.9193 - val_loss: 0.2031 - val_accuracy: 0.9272\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.9123 - val_loss: 0.2023 - val_accuracy: 0.9272\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9234 - val_loss: 0.2016 - val_accuracy: 0.9272\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.9196 - val_loss: 0.2009 - val_accuracy: 0.9272\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2147 - accuracy: 0.9273 - val_loss: 0.2006 - val_accuracy: 0.9272\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1834 - accuracy: 0.9325 - val_loss: 0.1997 - val_accuracy: 0.9299\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2041 - accuracy: 0.9272 - val_loss: 0.1992 - val_accuracy: 0.9272\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.9291 - val_loss: 0.1983 - val_accuracy: 0.9299\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9276 - val_loss: 0.1974 - val_accuracy: 0.9286\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2168 - accuracy: 0.9295 - val_loss: 0.1966 - val_accuracy: 0.9286\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1909 - accuracy: 0.9373 - val_loss: 0.1957 - val_accuracy: 0.9312\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2412 - accuracy: 0.9180 - val_loss: 0.1955 - val_accuracy: 0.9286\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1891 - accuracy: 0.9361 - val_loss: 0.1939 - val_accuracy: 0.9312\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.9275 - val_loss: 0.1931 - val_accuracy: 0.9312\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2122 - accuracy: 0.9276 - val_loss: 0.1921 - val_accuracy: 0.9312\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1852 - accuracy: 0.9378 - val_loss: 0.1912 - val_accuracy: 0.9299\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1713 - accuracy: 0.9424 - val_loss: 0.1902 - val_accuracy: 0.9339\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2054 - accuracy: 0.9226 - val_loss: 0.1902 - val_accuracy: 0.9312\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2000 - accuracy: 0.9189 - val_loss: 0.1887 - val_accuracy: 0.9299\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2039 - accuracy: 0.9295 - val_loss: 0.1877 - val_accuracy: 0.9325\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1880 - accuracy: 0.9321 - val_loss: 0.1871 - val_accuracy: 0.9325\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1934 - accuracy: 0.9247 - val_loss: 0.1865 - val_accuracy: 0.9325\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.9335 - val_loss: 0.1861 - val_accuracy: 0.9339\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1702 - accuracy: 0.9454 - val_loss: 0.1850 - val_accuracy: 0.9339\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9254 - val_loss: 0.1843 - val_accuracy: 0.9299\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2065 - accuracy: 0.9237 - val_loss: 0.1838 - val_accuracy: 0.9325\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.9291 - val_loss: 0.1831 - val_accuracy: 0.9312\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1889 - accuracy: 0.9271 - val_loss: 0.1826 - val_accuracy: 0.9325\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1843 - accuracy: 0.9322 - val_loss: 0.1818 - val_accuracy: 0.9339\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1862 - accuracy: 0.9299 - val_loss: 0.1811 - val_accuracy: 0.9325\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9338 - val_loss: 0.1801 - val_accuracy: 0.9339\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.9199 - val_loss: 0.1798 - val_accuracy: 0.9312\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.9247 - val_loss: 0.1794 - val_accuracy: 0.9339\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1496 - accuracy: 0.9404 - val_loss: 0.1783 - val_accuracy: 0.9339\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1589 - accuracy: 0.9334 - val_loss: 0.1773 - val_accuracy: 0.9312\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1859 - accuracy: 0.9289 - val_loss: 0.1763 - val_accuracy: 0.9325\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1536 - accuracy: 0.9467 - val_loss: 0.1769 - val_accuracy: 0.9312\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1667 - accuracy: 0.9386 - val_loss: 0.1757 - val_accuracy: 0.9325\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1768 - accuracy: 0.9313 - val_loss: 0.1747 - val_accuracy: 0.9312\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1515 - accuracy: 0.9446 - val_loss: 0.1741 - val_accuracy: 0.9325\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1731 - accuracy: 0.9430 - val_loss: 0.1743 - val_accuracy: 0.9339\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2097 - accuracy: 0.9133 - val_loss: 0.1724 - val_accuracy: 0.9312\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_28_input'), name='dense_28_input', description=\"created by layer 'dense_28_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "8\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 756, 4)            20        \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 756, 20)           100       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 756, 20)           420       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 756, 1)            21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_32_input'), name='dense_32_input', description=\"created by layer 'dense_32_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_32_input'), name='dense_32_input', description=\"created by layer 'dense_32_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      " 1/38 [..............................] - ETA: 26s - loss: 0.7648 - accuracy: 0.0500WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_32_input'), name='dense_32_input', description=\"created by layer 'dense_32_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 0.7371 - accuracy: 0.2947 - val_loss: 0.6526 - val_accuracy: 0.6653\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6344 - accuracy: 0.6726 - val_loss: 0.5518 - val_accuracy: 0.8148\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.8394 - val_loss: 0.4362 - val_accuracy: 0.8704\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8987 - val_loss: 0.3516 - val_accuracy: 0.8929\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3543 - accuracy: 0.8929 - val_loss: 0.3067 - val_accuracy: 0.9021\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3052 - accuracy: 0.8852 - val_loss: 0.2835 - val_accuracy: 0.9034\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2842 - accuracy: 0.9036 - val_loss: 0.2731 - val_accuracy: 0.9048\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3009 - accuracy: 0.9031 - val_loss: 0.2649 - val_accuracy: 0.9061\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2351 - accuracy: 0.8980 - val_loss: 0.2583 - val_accuracy: 0.9114\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2975 - accuracy: 0.9036 - val_loss: 0.2554 - val_accuracy: 0.9127\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2486 - accuracy: 0.9249 - val_loss: 0.2518 - val_accuracy: 0.9140\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2294 - accuracy: 0.9203 - val_loss: 0.2500 - val_accuracy: 0.9153\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.9125 - val_loss: 0.2480 - val_accuracy: 0.9140\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.9169 - val_loss: 0.2465 - val_accuracy: 0.9153\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.9189 - val_loss: 0.2450 - val_accuracy: 0.9153\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2515 - accuracy: 0.9078 - val_loss: 0.2439 - val_accuracy: 0.9140\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2327 - accuracy: 0.9246 - val_loss: 0.2426 - val_accuracy: 0.9167\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.9245 - val_loss: 0.2417 - val_accuracy: 0.9180\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2345 - accuracy: 0.9221 - val_loss: 0.2406 - val_accuracy: 0.9180\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2478 - accuracy: 0.9178 - val_loss: 0.2398 - val_accuracy: 0.9180\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2204 - accuracy: 0.9240 - val_loss: 0.2390 - val_accuracy: 0.9193\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2341 - accuracy: 0.9191 - val_loss: 0.2382 - val_accuracy: 0.9180\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.9112 - val_loss: 0.2376 - val_accuracy: 0.9193\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2371 - accuracy: 0.9259 - val_loss: 0.2369 - val_accuracy: 0.9193\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2575 - accuracy: 0.9202 - val_loss: 0.2364 - val_accuracy: 0.9193\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2206 - accuracy: 0.9232 - val_loss: 0.2356 - val_accuracy: 0.9193\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2201 - accuracy: 0.9253 - val_loss: 0.2350 - val_accuracy: 0.9193\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3119 - accuracy: 0.8920 - val_loss: 0.2344 - val_accuracy: 0.9193\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2220 - accuracy: 0.9188 - val_loss: 0.2338 - val_accuracy: 0.9193\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.9220 - val_loss: 0.2333 - val_accuracy: 0.9193\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2334 - accuracy: 0.9282 - val_loss: 0.2327 - val_accuracy: 0.9193\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2565 - accuracy: 0.9104 - val_loss: 0.2323 - val_accuracy: 0.9180\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2186 - accuracy: 0.9173 - val_loss: 0.2318 - val_accuracy: 0.9193\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2309 - accuracy: 0.9168 - val_loss: 0.2313 - val_accuracy: 0.9193\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.9380 - val_loss: 0.2309 - val_accuracy: 0.9193\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2817 - accuracy: 0.9054 - val_loss: 0.2302 - val_accuracy: 0.9193\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2377 - accuracy: 0.9138 - val_loss: 0.2296 - val_accuracy: 0.9193\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2542 - accuracy: 0.9067 - val_loss: 0.2291 - val_accuracy: 0.9193\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1999 - accuracy: 0.9242 - val_loss: 0.2288 - val_accuracy: 0.9193\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1911 - accuracy: 0.9323 - val_loss: 0.2282 - val_accuracy: 0.9193\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2043 - accuracy: 0.9221 - val_loss: 0.2278 - val_accuracy: 0.9180\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2522 - accuracy: 0.9100 - val_loss: 0.2274 - val_accuracy: 0.9206\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9270 - val_loss: 0.2268 - val_accuracy: 0.9206\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.9119 - val_loss: 0.2263 - val_accuracy: 0.9193\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.9090 - val_loss: 0.2258 - val_accuracy: 0.9180\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2292 - accuracy: 0.9094 - val_loss: 0.2257 - val_accuracy: 0.9206\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2297 - accuracy: 0.9228 - val_loss: 0.2249 - val_accuracy: 0.9180\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2379 - accuracy: 0.9154 - val_loss: 0.2244 - val_accuracy: 0.9193\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2349 - accuracy: 0.9184 - val_loss: 0.2240 - val_accuracy: 0.9206\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2688 - accuracy: 0.9068 - val_loss: 0.2238 - val_accuracy: 0.9193\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2451 - accuracy: 0.9200 - val_loss: 0.2232 - val_accuracy: 0.9206\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9264 - val_loss: 0.2226 - val_accuracy: 0.9193\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1947 - accuracy: 0.9279 - val_loss: 0.2223 - val_accuracy: 0.9206\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2317 - accuracy: 0.9150 - val_loss: 0.2217 - val_accuracy: 0.9193\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3004 - accuracy: 0.9124 - val_loss: 0.2214 - val_accuracy: 0.9206\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2303 - accuracy: 0.9183 - val_loss: 0.2209 - val_accuracy: 0.9180\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2165 - accuracy: 0.9178 - val_loss: 0.2202 - val_accuracy: 0.9193\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2383 - accuracy: 0.9162 - val_loss: 0.2197 - val_accuracy: 0.9193\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2642 - accuracy: 0.9021 - val_loss: 0.2192 - val_accuracy: 0.9193\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.9126 - val_loss: 0.2183 - val_accuracy: 0.9193\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2289 - accuracy: 0.9207 - val_loss: 0.2176 - val_accuracy: 0.9193\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.8994 - val_loss: 0.2169 - val_accuracy: 0.9193\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.9296 - val_loss: 0.2158 - val_accuracy: 0.9193\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2475 - accuracy: 0.9107 - val_loss: 0.2146 - val_accuracy: 0.9193\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2251 - accuracy: 0.9134 - val_loss: 0.2132 - val_accuracy: 0.9193\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2663 - accuracy: 0.9183 - val_loss: 0.2112 - val_accuracy: 0.9206\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2189 - accuracy: 0.9079 - val_loss: 0.2089 - val_accuracy: 0.9193\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2173 - accuracy: 0.9183 - val_loss: 0.2047 - val_accuracy: 0.9193\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.9222 - val_loss: 0.2018 - val_accuracy: 0.9193\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2324 - accuracy: 0.9087 - val_loss: 0.1980 - val_accuracy: 0.9193\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1993 - accuracy: 0.9252 - val_loss: 0.1944 - val_accuracy: 0.9325\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1968 - accuracy: 0.9262 - val_loss: 0.1899 - val_accuracy: 0.9339\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1973 - accuracy: 0.9341 - val_loss: 0.1856 - val_accuracy: 0.9339\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2204 - accuracy: 0.9220 - val_loss: 0.1825 - val_accuracy: 0.9365\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2215 - accuracy: 0.9229 - val_loss: 0.1769 - val_accuracy: 0.9444\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1741 - accuracy: 0.9477 - val_loss: 0.1725 - val_accuracy: 0.9458\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1897 - accuracy: 0.9300 - val_loss: 0.1709 - val_accuracy: 0.9458\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1856 - accuracy: 0.9378 - val_loss: 0.1652 - val_accuracy: 0.9444\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1611 - accuracy: 0.9455 - val_loss: 0.1624 - val_accuracy: 0.9444\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1769 - accuracy: 0.9343 - val_loss: 0.1587 - val_accuracy: 0.9458\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1415 - accuracy: 0.9518 - val_loss: 0.1559 - val_accuracy: 0.9444\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1865 - accuracy: 0.9432 - val_loss: 0.1525 - val_accuracy: 0.9458\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1629 - accuracy: 0.9436 - val_loss: 0.1505 - val_accuracy: 0.9471\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9401 - val_loss: 0.1476 - val_accuracy: 0.9458\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1300 - accuracy: 0.9500 - val_loss: 0.1465 - val_accuracy: 0.9511\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1465 - accuracy: 0.9627 - val_loss: 0.1426 - val_accuracy: 0.9497\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.9489 - val_loss: 0.1397 - val_accuracy: 0.9497\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1601 - accuracy: 0.9462 - val_loss: 0.1374 - val_accuracy: 0.9537\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1290 - accuracy: 0.9516 - val_loss: 0.1356 - val_accuracy: 0.9537\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1385 - accuracy: 0.9560 - val_loss: 0.1332 - val_accuracy: 0.9524\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1197 - accuracy: 0.9627 - val_loss: 0.1316 - val_accuracy: 0.9590\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1429 - accuracy: 0.9603 - val_loss: 0.1282 - val_accuracy: 0.9577\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1434 - accuracy: 0.9559 - val_loss: 0.1259 - val_accuracy: 0.9537\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1228 - accuracy: 0.9577 - val_loss: 0.1242 - val_accuracy: 0.9537\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1223 - accuracy: 0.9583 - val_loss: 0.1211 - val_accuracy: 0.9577\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1166 - accuracy: 0.9599 - val_loss: 0.1195 - val_accuracy: 0.9643\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1126 - accuracy: 0.9665 - val_loss: 0.1165 - val_accuracy: 0.9590\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1061 - accuracy: 0.9627 - val_loss: 0.1145 - val_accuracy: 0.9630\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.9702 - val_loss: 0.1145 - val_accuracy: 0.9669\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9707 - val_loss: 0.1104 - val_accuracy: 0.9616\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_32_input'), name='dense_32_input', description=\"created by layer 'dense_32_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "9\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 756, 4)            20        \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 756, 20)           100       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 756, 20)           420       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 756, 1)            21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_36_input'), name='dense_36_input', description=\"created by layer 'dense_36_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_36_input'), name='dense_36_input', description=\"created by layer 'dense_36_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      " 1/38 [..............................] - ETA: 26s - loss: 0.7421 - accuracy: 0.2000WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_36_input'), name='dense_36_input', description=\"created by layer 'dense_36_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 0.7142 - accuracy: 0.3250 - val_loss: 0.6332 - val_accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6089 - accuracy: 0.8248 - val_loss: 0.5298 - val_accuracy: 0.8690\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5003 - accuracy: 0.8778 - val_loss: 0.4305 - val_accuracy: 0.8823\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4113 - accuracy: 0.8934 - val_loss: 0.3700 - val_accuracy: 0.8915\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.8806 - val_loss: 0.3298 - val_accuracy: 0.9048\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3050 - accuracy: 0.9086 - val_loss: 0.2996 - val_accuracy: 0.9087\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.9059 - val_loss: 0.2794 - val_accuracy: 0.9114\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.9020 - val_loss: 0.2673 - val_accuracy: 0.9114\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.9020 - val_loss: 0.2594 - val_accuracy: 0.9114\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.9163 - val_loss: 0.2535 - val_accuracy: 0.9114\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2498 - accuracy: 0.9149 - val_loss: 0.2483 - val_accuracy: 0.9101\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2320 - accuracy: 0.9264 - val_loss: 0.2435 - val_accuracy: 0.9087\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2239 - accuracy: 0.9114 - val_loss: 0.2375 - val_accuracy: 0.9074\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2255 - accuracy: 0.9097 - val_loss: 0.2333 - val_accuracy: 0.9087\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2157 - accuracy: 0.9152 - val_loss: 0.2294 - val_accuracy: 0.9101\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2172 - accuracy: 0.9185 - val_loss: 0.2260 - val_accuracy: 0.9167\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2215 - accuracy: 0.9213 - val_loss: 0.2227 - val_accuracy: 0.9167\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2613 - accuracy: 0.9140 - val_loss: 0.2197 - val_accuracy: 0.9167\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2140 - accuracy: 0.9195 - val_loss: 0.2173 - val_accuracy: 0.9193\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2506 - accuracy: 0.9237 - val_loss: 0.2141 - val_accuracy: 0.9272\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9294 - val_loss: 0.2117 - val_accuracy: 0.9286\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.9318 - val_loss: 0.2095 - val_accuracy: 0.9272\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2084 - accuracy: 0.9284 - val_loss: 0.2067 - val_accuracy: 0.9286\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1721 - accuracy: 0.9565 - val_loss: 0.2051 - val_accuracy: 0.9312\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2244 - accuracy: 0.9149 - val_loss: 0.2030 - val_accuracy: 0.9312\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1870 - accuracy: 0.9431 - val_loss: 0.2004 - val_accuracy: 0.9339\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2145 - accuracy: 0.9212 - val_loss: 0.1982 - val_accuracy: 0.9325\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9313 - val_loss: 0.1961 - val_accuracy: 0.9339\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2151 - accuracy: 0.9253 - val_loss: 0.1940 - val_accuracy: 0.9339\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1956 - accuracy: 0.9373 - val_loss: 0.1922 - val_accuracy: 0.9339\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1985 - accuracy: 0.9318 - val_loss: 0.1898 - val_accuracy: 0.9352\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1772 - accuracy: 0.9317 - val_loss: 0.1878 - val_accuracy: 0.9378\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1670 - accuracy: 0.9446 - val_loss: 0.1857 - val_accuracy: 0.9365\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1772 - accuracy: 0.9404 - val_loss: 0.1831 - val_accuracy: 0.9378\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1803 - accuracy: 0.9381 - val_loss: 0.1814 - val_accuracy: 0.9378\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.9386 - val_loss: 0.1788 - val_accuracy: 0.9378\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1777 - accuracy: 0.9393 - val_loss: 0.1769 - val_accuracy: 0.9392\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1494 - accuracy: 0.9457 - val_loss: 0.1744 - val_accuracy: 0.9378\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9408 - val_loss: 0.1726 - val_accuracy: 0.9378\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1798 - accuracy: 0.9360 - val_loss: 0.1692 - val_accuracy: 0.9418\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1667 - accuracy: 0.9386 - val_loss: 0.1667 - val_accuracy: 0.9418\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1965 - accuracy: 0.9269 - val_loss: 0.1659 - val_accuracy: 0.9418\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1790 - accuracy: 0.9379 - val_loss: 0.1614 - val_accuracy: 0.9431\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.9346 - val_loss: 0.1593 - val_accuracy: 0.9431\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1907 - accuracy: 0.9275 - val_loss: 0.1565 - val_accuracy: 0.9431\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1957 - accuracy: 0.9309 - val_loss: 0.1534 - val_accuracy: 0.9444\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9577 - val_loss: 0.1506 - val_accuracy: 0.9431\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1493 - accuracy: 0.9505 - val_loss: 0.1486 - val_accuracy: 0.9431\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1434 - accuracy: 0.9497 - val_loss: 0.1449 - val_accuracy: 0.9471\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1475 - accuracy: 0.9529 - val_loss: 0.1432 - val_accuracy: 0.9458\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9470 - val_loss: 0.1396 - val_accuracy: 0.9497\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1343 - accuracy: 0.9551 - val_loss: 0.1373 - val_accuracy: 0.9497\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1146 - accuracy: 0.9614 - val_loss: 0.1353 - val_accuracy: 0.9524\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1236 - accuracy: 0.9572 - val_loss: 0.1305 - val_accuracy: 0.9511\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1464 - accuracy: 0.9504 - val_loss: 0.1288 - val_accuracy: 0.9511\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1483 - accuracy: 0.9369 - val_loss: 0.1252 - val_accuracy: 0.9511\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1322 - accuracy: 0.9476 - val_loss: 0.1217 - val_accuracy: 0.9524\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0988 - accuracy: 0.9633 - val_loss: 0.1233 - val_accuracy: 0.9590\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1281 - accuracy: 0.9513 - val_loss: 0.1206 - val_accuracy: 0.9524\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1222 - accuracy: 0.9568 - val_loss: 0.1140 - val_accuracy: 0.9603\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1193 - accuracy: 0.9508 - val_loss: 0.1115 - val_accuracy: 0.9616\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1045 - accuracy: 0.9612 - val_loss: 0.1102 - val_accuracy: 0.9590\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1135 - accuracy: 0.9631 - val_loss: 0.1074 - val_accuracy: 0.9616\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1380 - accuracy: 0.9507 - val_loss: 0.1063 - val_accuracy: 0.9630\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0930 - accuracy: 0.9721 - val_loss: 0.1032 - val_accuracy: 0.9643\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1219 - accuracy: 0.9574 - val_loss: 0.1030 - val_accuracy: 0.9656\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1103 - accuracy: 0.9554 - val_loss: 0.0990 - val_accuracy: 0.9643\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0790 - accuracy: 0.9703 - val_loss: 0.0999 - val_accuracy: 0.9696\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0944 - accuracy: 0.9687 - val_loss: 0.0996 - val_accuracy: 0.9683\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1026 - accuracy: 0.9641 - val_loss: 0.0952 - val_accuracy: 0.9683\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1077 - accuracy: 0.9636 - val_loss: 0.0939 - val_accuracy: 0.9683\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.9726 - val_loss: 0.0913 - val_accuracy: 0.9722\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0948 - accuracy: 0.9689 - val_loss: 0.0916 - val_accuracy: 0.9709\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0880 - accuracy: 0.9624 - val_loss: 0.0979 - val_accuracy: 0.9669\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0989 - accuracy: 0.9647 - val_loss: 0.0873 - val_accuracy: 0.9709\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0689 - accuracy: 0.9756 - val_loss: 0.0862 - val_accuracy: 0.9709\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1221 - accuracy: 0.9509 - val_loss: 0.0850 - val_accuracy: 0.9709\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9848 - val_loss: 0.0847 - val_accuracy: 0.9722\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0869 - accuracy: 0.9737 - val_loss: 0.0837 - val_accuracy: 0.9722\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0715 - accuracy: 0.9764 - val_loss: 0.0832 - val_accuracy: 0.9722\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1058 - accuracy: 0.9611 - val_loss: 0.0834 - val_accuracy: 0.9722\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9822 - val_loss: 0.0841 - val_accuracy: 0.9749\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.9803 - val_loss: 0.0802 - val_accuracy: 0.9709\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.9756 - val_loss: 0.0797 - val_accuracy: 0.9709\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0787 - accuracy: 0.9755 - val_loss: 0.0791 - val_accuracy: 0.9722\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.9695 - val_loss: 0.0798 - val_accuracy: 0.9735\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9821 - val_loss: 0.0797 - val_accuracy: 0.9762\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.9814 - val_loss: 0.0779 - val_accuracy: 0.9735\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.9632 - val_loss: 0.0772 - val_accuracy: 0.9722\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0697 - accuracy: 0.9761 - val_loss: 0.0771 - val_accuracy: 0.9722\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.9735 - val_loss: 0.0774 - val_accuracy: 0.9749\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.0880 - accuracy: 0.9747 - val_loss: 0.0763 - val_accuracy: 0.9722\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.0952 - accuracy: 0.9681 - val_loss: 0.0767 - val_accuracy: 0.9749\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9727 - val_loss: 0.0757 - val_accuracy: 0.9709\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0930 - accuracy: 0.9671 - val_loss: 0.0785 - val_accuracy: 0.9735\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0681 - accuracy: 0.9732 - val_loss: 0.0760 - val_accuracy: 0.9749\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9763 - val_loss: 0.0749 - val_accuracy: 0.9722\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0773 - accuracy: 0.9680 - val_loss: 0.0754 - val_accuracy: 0.9749\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0786 - accuracy: 0.9741 - val_loss: 0.0745 - val_accuracy: 0.9722\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9773 - val_loss: 0.0757 - val_accuracy: 0.9735\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 756, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 756, 4), dtype=tf.float32, name='dense_36_input'), name='dense_36_input', description=\"created by layer 'dense_36_input'\"), but it was called on an input with incompatible shape (None, 4).\n"
     ]
    }
   ],
   "source": [
    "df_columns = ['Precision', 'Recall', 'F1 score', 'AUC', 'Train time', 'Test time']\n",
    "t_enm_mlp_df = pd.DataFrame(columns=df_columns)\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    enm_mlp, model = ensemble_mlp()\n",
    "    t_enm_mlp_df = pd.concat([t_enm_mlp_df, enm_mlp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble mlp\n",
      "[0.9347, 0.9225, 0.9278, 0.9784, 18.1623, 5.9506]\n",
      "[0.0153, 0.0236, 0.0197, 0.008, 3.7507, 0.0604]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Train time</th>\n",
       "      <th>Test time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9127</td>\n",
       "      <td>0.9017</td>\n",
       "      <td>0.9068</td>\n",
       "      <td>0.9689</td>\n",
       "      <td>28.3825</td>\n",
       "      <td>6.1072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9247</td>\n",
       "      <td>0.9234</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>14.8871</td>\n",
       "      <td>5.9066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9243</td>\n",
       "      <td>0.9018</td>\n",
       "      <td>0.9116</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>15.5696</td>\n",
       "      <td>5.9029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9491</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>0.9477</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>16.5245</td>\n",
       "      <td>5.9096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9434</td>\n",
       "      <td>0.9492</td>\n",
       "      <td>0.9462</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>17.2114</td>\n",
       "      <td>5.9629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.9206</td>\n",
       "      <td>0.8942</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>0.9662</td>\n",
       "      <td>18.3668</td>\n",
       "      <td>5.9325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9496</td>\n",
       "      <td>0.9385</td>\n",
       "      <td>0.9437</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>17.6673</td>\n",
       "      <td>5.9777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.9215</td>\n",
       "      <td>0.8899</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>0.9684</td>\n",
       "      <td>17.7631</td>\n",
       "      <td>5.9203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9470</td>\n",
       "      <td>0.9325</td>\n",
       "      <td>0.9391</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>17.8507</td>\n",
       "      <td>5.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9508</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>17.4003</td>\n",
       "      <td>5.9530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Recall  F1 score     AUC  Train time  Test time\n",
       "0     0.9127  0.9017    0.9068  0.9689     28.3825     6.1072\n",
       "1     0.9247  0.9234    0.9240  0.9781     14.8871     5.9066\n",
       "2     0.9243  0.9018    0.9116  0.9777     15.5696     5.9029\n",
       "3     0.9491  0.9464    0.9477  0.9843     16.5245     5.9096\n",
       "4     0.9434  0.9492    0.9462  0.9840     17.2114     5.9629\n",
       "5     0.9206  0.8942    0.9054  0.9662     18.3668     5.9325\n",
       "6     0.9496  0.9385    0.9437  0.9848     17.6673     5.9777\n",
       "7     0.9215  0.8899    0.9029  0.9684     17.7631     5.9203\n",
       "8     0.9470  0.9325    0.9391  0.9821     17.8507     5.9333\n",
       "9     0.9541  0.9478    0.9508  0.9893     17.4003     5.9530"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Ensemble mlp')\n",
    "metrics_name = ['Precision', 'Recall', 'F1 score', 'AUC', 'Train time', 'Test time']\n",
    "mean_mlp = []\n",
    "std_mlp = []\n",
    "for each in metrics_name:\n",
    "    mean_mlp.append(round(t_enm_mlp_df[each].mean(), ndigits=4))\n",
    "    std_mlp.append(round(t_enm_mlp_df[each].std(), ndigits=4))\n",
    "print(mean_mlp)\n",
    "print(std_mlp)\n",
    "t_enm_mlp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
